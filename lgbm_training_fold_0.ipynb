{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_full_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = load(\"selected_features.pkl\")\n",
    "\n",
    "target = \"target\"\n",
    "\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "cat_features = [feature for feature in cat_features if feature in features]\n",
    "            \n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fold = 0\n",
    "\n",
    "kfold = StratifiedKFold(\n",
    "    n_splits=n_folds, \n",
    "    shuffle=True, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "    \n",
    "    if fold == target_fold:\n",
    "        break\n",
    "\n",
    "x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(\"../ckpt/lgbm_seed_{}\".format(seed))\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "save_path = os.path.join(save_folder, \"fold_{}\".format(target_fold))\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 0 with 1950 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.135973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211677\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "iteration 0, score= 0.68021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100, score= 0.75132\n",
      "iteration 200, score= 0.75479\n",
      "iteration 300, score= 0.75688\n",
      "iteration 400, score= 0.75908\n",
      "[500]\ttraining's binary_logloss: 0.337211\ttraining's amex_metric: 0.778376\tvalid_1's binary_logloss: 0.340889\tvalid_1's amex_metric: 0.761554\n",
      "iteration 500, score= 0.76184\n",
      "iteration 600, score= 0.76396\n",
      "iteration 700, score= 0.76605\n",
      "iteration 800, score= 0.76896\n",
      "iteration 900, score= 0.77170\n",
      "[1000]\ttraining's binary_logloss: 0.246119\ttraining's amex_metric: 0.795462\tvalid_1's binary_logloss: 0.253861\tvalid_1's amex_metric: 0.773459\n",
      "iteration 1000, score= 0.77344\n",
      "iteration 1100, score= 0.77553\n",
      "iteration 1200, score= 0.77711\n",
      "iteration 1300, score= 0.77770\n",
      "iteration 1400, score= 0.77919\n",
      "[1500]\ttraining's binary_logloss: 0.222074\ttraining's amex_metric: 0.809339\tvalid_1's binary_logloss: 0.233818\tvalid_1's amex_metric: 0.779861\n",
      "iteration 1500, score= 0.78003\n",
      "iteration 1600, score= 0.78121\n",
      "iteration 1700, score= 0.78238\n",
      "iteration 1800, score= 0.78347\n",
      "iteration 1900, score= 0.78433\n",
      "[2000]\ttraining's binary_logloss: 0.20847\ttraining's amex_metric: 0.821426\tvalid_1's binary_logloss: 0.225029\tvalid_1's amex_metric: 0.785301\n",
      "iteration 2000, score= 0.78549\n",
      "iteration 2100, score= 0.78566\n",
      "iteration 2200, score= 0.78570\n",
      "iteration 2300, score= 0.78649\n",
      "iteration 2400, score= 0.78672\n",
      "[2500]\ttraining's binary_logloss: 0.201538\ttraining's amex_metric: 0.831494\tvalid_1's binary_logloss: 0.222168\tvalid_1's amex_metric: 0.78727\n",
      "iteration 2500, score= 0.78727\n",
      "iteration 2600, score= 0.78778\n",
      "iteration 2700, score= 0.78842\n",
      "iteration 2800, score= 0.78792\n",
      "iteration 2900, score= 0.78884\n",
      "[3000]\ttraining's binary_logloss: 0.194679\ttraining's amex_metric: 0.840971\tvalid_1's binary_logloss: 0.219885\tvalid_1's amex_metric: 0.789262\n",
      "iteration 3000, score= 0.78928\n",
      "iteration 3100, score= 0.78983\n",
      "High Score: iteration 3101, score=0.79003\n",
      "High Score: iteration 3140, score=0.79007\n",
      "High Score: iteration 3142, score=0.79011\n",
      "High Score: iteration 3143, score=0.79011\n",
      "High Score: iteration 3145, score=0.79012\n",
      "High Score: iteration 3146, score=0.79018\n",
      "High Score: iteration 3180, score=0.79038\n",
      "High Score: iteration 3189, score=0.79057\n",
      "High Score: iteration 3193, score=0.79060\n",
      "High Score: iteration 3194, score=0.79060\n",
      "High Score: iteration 3195, score=0.79060\n",
      "High Score: iteration 3196, score=0.79060\n",
      "iteration 3200, score= 0.79018\n",
      "High Score: iteration 3238, score=0.79063\n",
      "High Score: iteration 3239, score=0.79063\n",
      "High Score: iteration 3240, score=0.79063\n",
      "High Score: iteration 3241, score=0.79067\n",
      "High Score: iteration 3242, score=0.79069\n",
      "High Score: iteration 3251, score=0.79077\n",
      "High Score: iteration 3252, score=0.79096\n",
      "High Score: iteration 3276, score=0.79100\n",
      "High Score: iteration 3278, score=0.79104\n",
      "High Score: iteration 3282, score=0.79104\n",
      "High Score: iteration 3284, score=0.79104\n",
      "High Score: iteration 3296, score=0.79110\n",
      "High Score: iteration 3298, score=0.79114\n",
      "High Score: iteration 3299, score=0.79114\n",
      "iteration 3300, score= 0.79116\n",
      "High Score: iteration 3300, score=0.79116\n",
      "High Score: iteration 3315, score=0.79119\n",
      "High Score: iteration 3343, score=0.79121\n",
      "High Score: iteration 3344, score=0.79121\n",
      "High Score: iteration 3363, score=0.79122\n",
      "High Score: iteration 3391, score=0.79141\n",
      "iteration 3400, score= 0.79133\n",
      "High Score: iteration 3401, score=0.79150\n",
      "[3500]\ttraining's binary_logloss: 0.188321\ttraining's amex_metric: 0.850519\tvalid_1's binary_logloss: 0.218384\tvalid_1's amex_metric: 0.791062\n",
      "iteration 3500, score= 0.79104\n",
      "iteration 3600, score= 0.79031\n",
      "iteration 3700, score= 0.79102\n",
      "iteration 3800, score= 0.79055\n",
      "iteration 3900, score= 0.79088\n",
      "[4000]\ttraining's binary_logloss: 0.182799\ttraining's amex_metric: 0.860321\tvalid_1's binary_logloss: 0.217626\tvalid_1's amex_metric: 0.790965\n",
      "iteration 4000, score= 0.79092\n",
      "iteration 4100, score= 0.79062\n",
      "iteration 4200, score= 0.79110\n",
      "High Score: iteration 4243, score=0.79154\n",
      "High Score: iteration 4252, score=0.79156\n",
      "High Score: iteration 4255, score=0.79156\n",
      "High Score: iteration 4256, score=0.79167\n",
      "High Score: iteration 4257, score=0.79171\n",
      "High Score: iteration 4258, score=0.79175\n",
      "High Score: iteration 4262, score=0.79182\n",
      "High Score: iteration 4266, score=0.79191\n",
      "High Score: iteration 4284, score=0.79194\n",
      "High Score: iteration 4285, score=0.79196\n",
      "iteration 4300, score= 0.79156\n",
      "iteration 4400, score= 0.79132\n",
      "[4500]\ttraining's binary_logloss: 0.177362\ttraining's amex_metric: 0.869344\tvalid_1's binary_logloss: 0.216921\tvalid_1's amex_metric: 0.791608\n",
      "iteration 4500, score= 0.79163\n",
      "High Score: iteration 4579, score=0.79206\n",
      "High Score: iteration 4590, score=0.79208\n",
      "High Score: iteration 4591, score=0.79208\n",
      "High Score: iteration 4592, score=0.79216\n",
      "iteration 4600, score= 0.79221\n",
      "High Score: iteration 4600, score=0.79221\n",
      "High Score: iteration 4673, score=0.79221\n",
      "High Score: iteration 4674, score=0.79228\n",
      "High Score: iteration 4685, score=0.79236\n",
      "High Score: iteration 4686, score=0.79238\n",
      "High Score: iteration 4696, score=0.79239\n",
      "iteration 4700, score= 0.79213\n",
      "iteration 4800, score= 0.79113\n",
      "iteration 4900, score= 0.79129\n",
      "[5000]\ttraining's binary_logloss: 0.172008\ttraining's amex_metric: 0.878385\tvalid_1's binary_logloss: 0.216383\tvalid_1's amex_metric: 0.791822\n",
      "iteration 5000, score= 0.79174\n",
      "iteration 5100, score= 0.79155\n",
      "iteration 5200, score= 0.79196\n",
      "iteration 5300, score= 0.79159\n",
      "iteration 5400, score= 0.79194\n",
      "[5500]\ttraining's binary_logloss: 0.167229\ttraining's amex_metric: 0.886574\tvalid_1's binary_logloss: 0.216019\tvalid_1's amex_metric: 0.791621\n",
      "iteration 5500, score= 0.79148\n",
      "iteration 5600, score= 0.79179\n",
      "iteration 5700, score= 0.79187\n",
      "iteration 5800, score= 0.79202\n",
      "High Score: iteration 5832, score=0.79244\n",
      "High Score: iteration 5838, score=0.79253\n",
      "iteration 5900, score= 0.79226\n",
      "[6000]\ttraining's binary_logloss: 0.163138\ttraining's amex_metric: 0.894095\tvalid_1's binary_logloss: 0.215826\tvalid_1's amex_metric: 0.792231\n",
      "iteration 6000, score= 0.79232\n",
      "High Score: iteration 6091, score=0.79254\n",
      "iteration 6100, score= 0.79244\n",
      "High Score: iteration 6121, score=0.79255\n",
      "High Score: iteration 6122, score=0.79263\n",
      "High Score: iteration 6182, score=0.79274\n",
      "iteration 6200, score= 0.79249\n",
      "iteration 6300, score= 0.79249\n",
      "iteration 6400, score= 0.79237\n",
      "High Score: iteration 6414, score=0.79278\n",
      "High Score: iteration 6415, score=0.79278\n",
      "High Score: iteration 6416, score=0.79278\n",
      "High Score: iteration 6417, score=0.79278\n",
      "High Score: iteration 6418, score=0.79282\n",
      "High Score: iteration 6420, score=0.79286\n",
      "High Score: iteration 6421, score=0.79286\n",
      "High Score: iteration 6424, score=0.79297\n",
      "High Score: iteration 6425, score=0.79301\n",
      "High Score: iteration 6426, score=0.79315\n",
      "High Score: iteration 6427, score=0.79315\n",
      "[6500]\ttraining's binary_logloss: 0.158856\ttraining's amex_metric: 0.901399\tvalid_1's binary_logloss: 0.215582\tvalid_1's amex_metric: 0.792555\n",
      "iteration 6500, score= 0.79262\n",
      "iteration 6600, score= 0.79262\n",
      "iteration 6700, score= 0.79300\n",
      "iteration 6800, score= 0.79280\n",
      "High Score: iteration 6839, score=0.79316\n",
      "High Score: iteration 6849, score=0.79326\n",
      "High Score: iteration 6877, score=0.79328\n",
      "High Score: iteration 6878, score=0.79328\n",
      "High Score: iteration 6879, score=0.79333\n",
      "High Score: iteration 6886, score=0.79335\n",
      "High Score: iteration 6887, score=0.79335\n",
      "High Score: iteration 6888, score=0.79335\n",
      "High Score: iteration 6889, score=0.79337\n",
      "iteration 6900, score= 0.79322\n",
      "High Score: iteration 6901, score=0.79339\n",
      "High Score: iteration 6902, score=0.79343\n",
      "High Score: iteration 6903, score=0.79345\n",
      "High Score: iteration 6904, score=0.79354\n",
      "High Score: iteration 6905, score=0.79354\n",
      "High Score: iteration 6906, score=0.79356\n",
      "High Score: iteration 6908, score=0.79362\n",
      "High Score: iteration 6909, score=0.79362\n",
      "[7000]\ttraining's binary_logloss: 0.153899\ttraining's amex_metric: 0.90957\tvalid_1's binary_logloss: 0.215298\tvalid_1's amex_metric: 0.793386\n",
      "iteration 7000, score= 0.79328\n",
      "iteration 7100, score= 0.79328\n",
      "iteration 7200, score= 0.79315\n",
      "iteration 7300, score= 0.79331\n",
      "High Score: iteration 7394, score=0.79374\n",
      "iteration 7400, score= 0.79353\n",
      "[7500]\ttraining's binary_logloss: 0.14918\ttraining's amex_metric: 0.9171\tvalid_1's binary_logloss: 0.215119\tvalid_1's amex_metric: 0.793535\n",
      "iteration 7500, score= 0.79360\n",
      "High Score: iteration 7527, score=0.79375\n",
      "High Score: iteration 7530, score=0.79375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Score: iteration 7534, score=0.79383\n",
      "High Score: iteration 7535, score=0.79394\n",
      "High Score: iteration 7537, score=0.79402\n",
      "iteration 7600, score= 0.79340\n",
      "iteration 7700, score= 0.79357\n",
      "High Score: iteration 7766, score=0.79405\n",
      "iteration 7800, score= 0.79352\n",
      "iteration 7900, score= 0.79340\n",
      "[8000]\ttraining's binary_logloss: 0.144986\ttraining's amex_metric: 0.92346\tvalid_1's binary_logloss: 0.215027\tvalid_1's amex_metric: 0.793782\n",
      "iteration 8000, score= 0.79374\n",
      "High Score: iteration 8090, score=0.79405\n",
      "High Score: iteration 8091, score=0.79407\n",
      "High Score: iteration 8093, score=0.79416\n",
      "High Score: iteration 8098, score=0.79418\n",
      "iteration 8100, score= 0.79407\n",
      "High Score: iteration 8132, score=0.79428\n",
      "High Score: iteration 8155, score=0.79430\n",
      "High Score: iteration 8160, score=0.79432\n",
      "High Score: iteration 8161, score=0.79434\n",
      "High Score: iteration 8165, score=0.79440\n",
      "High Score: iteration 8166, score=0.79451\n",
      "High Score: iteration 8189, score=0.79453\n",
      "High Score: iteration 8193, score=0.79455\n",
      "High Score: iteration 8195, score=0.79455\n",
      "High Score: iteration 8198, score=0.79459\n",
      "iteration 8200, score= 0.79459\n",
      "High Score: iteration 8207, score=0.79461\n",
      "High Score: iteration 8257, score=0.79464\n",
      "High Score: iteration 8259, score=0.79466\n",
      "High Score: iteration 8260, score=0.79466\n",
      "High Score: iteration 8261, score=0.79477\n",
      "High Score: iteration 8262, score=0.79479\n",
      "iteration 8300, score= 0.79441\n",
      "iteration 8400, score= 0.79454\n",
      "[8500]\ttraining's binary_logloss: 0.141413\ttraining's amex_metric: 0.929942\tvalid_1's binary_logloss: 0.214992\tvalid_1's amex_metric: 0.794593\n",
      "iteration 8500, score= 0.79459\n",
      "iteration 8600, score= 0.79480\n",
      "High Score: iteration 8600, score=0.79480\n",
      "High Score: iteration 8601, score=0.79496\n",
      "High Score: iteration 8603, score=0.79498\n",
      "iteration 8700, score= 0.79462\n",
      "High Score: iteration 8742, score=0.79503\n",
      "High Score: iteration 8743, score=0.79503\n",
      "iteration 8800, score= 0.79478\n",
      "High Score: iteration 8892, score=0.79504\n",
      "High Score: iteration 8893, score=0.79506\n",
      "High Score: iteration 8895, score=0.79506\n",
      "High Score: iteration 8898, score=0.79506\n",
      "High Score: iteration 8899, score=0.79508\n",
      "iteration 8900, score= 0.79512\n",
      "High Score: iteration 8900, score=0.79512\n",
      "High Score: iteration 8917, score=0.79513\n",
      "High Score: iteration 8918, score=0.79523\n",
      "High Score: iteration 8922, score=0.79534\n",
      "High Score: iteration 8925, score=0.79534\n",
      "High Score: iteration 8960, score=0.79540\n",
      "[9000]\ttraining's binary_logloss: 0.137374\ttraining's amex_metric: 0.936139\tvalid_1's binary_logloss: 0.214961\tvalid_1's amex_metric: 0.795163\n",
      "iteration 9000, score= 0.79516\n",
      "High Score: iteration 9008, score=0.79544\n",
      "High Score: iteration 9009, score=0.79544\n",
      "iteration 9100, score= 0.79517\n",
      "High Score: iteration 9189, score=0.79547\n",
      "High Score: iteration 9190, score=0.79548\n",
      "High Score: iteration 9193, score=0.79548\n",
      "High Score: iteration 9197, score=0.79548\n",
      "iteration 9200, score= 0.79539\n",
      "iteration 9300, score= 0.79493\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50)\n",
    "print(\"Training fold {} with {} features...\".format(target_fold, len(features)))\n",
    "\n",
    "global max_score \n",
    "max_score = 0.797\n",
    "\n",
    "def save_model():\n",
    "    def callback(env):\n",
    "        global max_score\n",
    "        iteration = env.iteration\n",
    "        score = env.evaluation_result_list[3][2]\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"iteration {}, score= {:.05f}\".format(iteration,score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            print(\"High Score: iteration {}, score={:.05f}\".format(iteration, score))\n",
    "            dump(env.model, os.path.join(save_path, \"{:.05f}.pkl\".format(score)))\n",
    "\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=20000,\n",
    "    valid_sets = [lgb_train, lgb_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval = 500,\n",
    "    feval = lgb_amex_metric,\n",
    "    callbacks=[save_model()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training(train):\n",
    "    \n",
    "#     # create a numpy array to store out of folds predictions\n",
    "#     oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "#     kfold = StratifiedKFold(\n",
    "#         n_splits=n_folds, \n",
    "#         shuffle=True, \n",
    "#         random_state=seed\n",
    "#     )\n",
    "    \n",
    "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "#         print(\"#\" * 50)\n",
    "#         print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "#         x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "#         y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "        \n",
    "#         lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "#         lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "#         model = lgb.train(\n",
    "#             params=params,\n",
    "#             train_set=lgb_train,\n",
    "#             num_boost_round=10500,\n",
    "#             valid_sets = [lgb_train, lgb_valid],\n",
    "#             early_stopping_rounds=100,\n",
    "#             verbose_eval = 500,\n",
    "#             feval = lgb_amex_metric\n",
    "#         )\n",
    "#         # save best model\n",
    "#         dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "#         # predict validation\n",
    "#         val_pred = model.predict(x_val)\n",
    "        \n",
    "#         # add to out of folds array\n",
    "#         oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "#         # compute fold metric\n",
    "#         score = amex_metric(y_val, val_pred)\n",
    "#         print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "#         del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "#         gc.collect()\n",
    "        \n",
    "#     # compute oof\n",
    "#     score = amex_metric(train[target], oof_predictions)\n",
    "#     print(\"oof score is {}\".format(score))\n",
    "    \n",
    "#     # create a dataframe to store out of folds predictions\n",
    "#     oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "#     oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "#     return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
