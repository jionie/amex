{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_full_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = load(\"selected_features.pkl\")\n",
    "\n",
    "target = \"target\"\n",
    "\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "cat_features = [feature for feature in cat_features if feature in features]\n",
    "            \n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fold = 2\n",
    "\n",
    "kfold = StratifiedKFold(\n",
    "    n_splits=n_folds, \n",
    "    shuffle=True, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "    \n",
    "    if fold == target_fold:\n",
    "        break\n",
    "\n",
    "x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, x_train, x_val, y_train, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(\"../ckpt/lgbm_seed_{}\".format(seed))\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "save_path = os.path.join(save_folder, \"fold_{}\".format(target_fold))\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 2 with 1950 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.394533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211535\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "iteration 0, score= 0.68221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100, score= 0.75586\n",
      "iteration 200, score= 0.75780\n",
      "iteration 300, score= 0.76081\n",
      "iteration 400, score= 0.76212\n",
      "[500]\ttraining's binary_logloss: 0.33714\ttraining's amex_metric: 0.777132\tvalid_1's binary_logloss: 0.340912\tvalid_1's amex_metric: 0.7647\n",
      "iteration 500, score= 0.76471\n",
      "iteration 600, score= 0.76656\n",
      "iteration 700, score= 0.76991\n",
      "iteration 800, score= 0.77232\n",
      "iteration 900, score= 0.77497\n",
      "[1000]\ttraining's binary_logloss: 0.24614\ttraining's amex_metric: 0.794756\tvalid_1's binary_logloss: 0.254017\tvalid_1's amex_metric: 0.776409\n",
      "iteration 1000, score= 0.77641\n",
      "iteration 1100, score= 0.77882\n",
      "iteration 1200, score= 0.78038\n",
      "iteration 1300, score= 0.78156\n",
      "iteration 1400, score= 0.78203\n",
      "[1500]\ttraining's binary_logloss: 0.222052\ttraining's amex_metric: 0.8082\tvalid_1's binary_logloss: 0.233728\tvalid_1's amex_metric: 0.782898\n",
      "iteration 1500, score= 0.78299\n",
      "iteration 1600, score= 0.78458\n",
      "iteration 1700, score= 0.78491\n",
      "iteration 1800, score= 0.78622\n",
      "iteration 1900, score= 0.78647\n",
      "[2000]\ttraining's binary_logloss: 0.208501\ttraining's amex_metric: 0.820978\tvalid_1's binary_logloss: 0.224928\tvalid_1's amex_metric: 0.787228\n",
      "iteration 2000, score= 0.78717\n",
      "iteration 2100, score= 0.78798\n",
      "iteration 2200, score= 0.78833\n",
      "iteration 2300, score= 0.78904\n",
      "iteration 2400, score= 0.78909\n",
      "[2500]\ttraining's binary_logloss: 0.201598\ttraining's amex_metric: 0.830841\tvalid_1's binary_logloss: 0.222038\tvalid_1's amex_metric: 0.790345\n",
      "iteration 2500, score= 0.79037\n",
      "iteration 2600, score= 0.79040\n",
      "iteration 2700, score= 0.79106\n",
      "iteration 2800, score= 0.79157\n",
      "iteration 2900, score= 0.79197\n",
      "[3000]\ttraining's binary_logloss: 0.194718\ttraining's amex_metric: 0.840711\tvalid_1's binary_logloss: 0.219817\tvalid_1's amex_metric: 0.791835\n",
      "iteration 3000, score= 0.79183\n",
      "iteration 3100, score= 0.79247\n",
      "iteration 3200, score= 0.79215\n",
      "High Score: iteration 3283, score=0.79301\n",
      "High Score: iteration 3287, score=0.79304\n",
      "High Score: iteration 3288, score=0.79304\n",
      "High Score: iteration 3289, score=0.79304\n",
      "High Score: iteration 3290, score=0.79304\n",
      "High Score: iteration 3292, score=0.79304\n",
      "High Score: iteration 3295, score=0.79307\n",
      "High Score: iteration 3298, score=0.79322\n",
      "High Score: iteration 3299, score=0.79324\n",
      "iteration 3300, score= 0.79324\n",
      "High Score: iteration 3300, score=0.79324\n",
      "iteration 3400, score= 0.79295\n",
      "High Score: iteration 3419, score=0.79327\n",
      "High Score: iteration 3420, score=0.79329\n",
      "High Score: iteration 3425, score=0.79332\n",
      "High Score: iteration 3486, score=0.79333\n",
      "High Score: iteration 3488, score=0.79335\n",
      "High Score: iteration 3493, score=0.79336\n",
      "High Score: iteration 3499, score=0.79336\n",
      "[3500]\ttraining's binary_logloss: 0.188389\ttraining's amex_metric: 0.850699\tvalid_1's binary_logloss: 0.218307\tvalid_1's amex_metric: 0.793364\n",
      "iteration 3500, score= 0.79330\n",
      "High Score: iteration 3501, score=0.79336\n",
      "High Score: iteration 3502, score=0.79336\n",
      "High Score: iteration 3506, score=0.79341\n",
      "High Score: iteration 3528, score=0.79357\n",
      "High Score: iteration 3529, score=0.79365\n",
      "High Score: iteration 3533, score=0.79366\n",
      "High Score: iteration 3595, score=0.79369\n",
      "iteration 3600, score= 0.79348\n",
      "High Score: iteration 3613, score=0.79370\n",
      "High Score: iteration 3614, score=0.79374\n",
      "High Score: iteration 3617, score=0.79376\n",
      "High Score: iteration 3624, score=0.79383\n",
      "High Score: iteration 3628, score=0.79385\n",
      "High Score: iteration 3629, score=0.79385\n",
      "High Score: iteration 3630, score=0.79385\n",
      "High Score: iteration 3631, score=0.79385\n",
      "High Score: iteration 3632, score=0.79387\n",
      "High Score: iteration 3638, score=0.79394\n",
      "High Score: iteration 3639, score=0.79394\n",
      "High Score: iteration 3640, score=0.79394\n",
      "High Score: iteration 3670, score=0.79400\n",
      "High Score: iteration 3671, score=0.79409\n",
      "High Score: iteration 3681, score=0.79410\n",
      "High Score: iteration 3684, score=0.79414\n",
      "High Score: iteration 3685, score=0.79414\n",
      "High Score: iteration 3686, score=0.79429\n",
      "High Score: iteration 3687, score=0.79431\n",
      "High Score: iteration 3688, score=0.79431\n",
      "High Score: iteration 3689, score=0.79431\n",
      "High Score: iteration 3690, score=0.79431\n",
      "High Score: iteration 3691, score=0.79435\n",
      "High Score: iteration 3695, score=0.79436\n",
      "High Score: iteration 3699, score=0.79449\n",
      "iteration 3700, score= 0.79447\n",
      "High Score: iteration 3703, score=0.79458\n",
      "High Score: iteration 3708, score=0.79473\n",
      "High Score: iteration 3709, score=0.79475\n",
      "High Score: iteration 3713, score=0.79477\n",
      "High Score: iteration 3714, score=0.79479\n",
      "High Score: iteration 3715, score=0.79484\n",
      "High Score: iteration 3716, score=0.79488\n",
      "High Score: iteration 3724, score=0.79490\n",
      "High Score: iteration 3725, score=0.79492\n",
      "iteration 3800, score= 0.79457\n",
      "High Score: iteration 3836, score=0.79495\n",
      "High Score: iteration 3839, score=0.79495\n",
      "High Score: iteration 3893, score=0.79495\n",
      "iteration 3900, score= 0.79474\n",
      "High Score: iteration 3924, score=0.79496\n",
      "High Score: iteration 3929, score=0.79496\n",
      "High Score: iteration 3930, score=0.79496\n",
      "High Score: iteration 3931, score=0.79496\n",
      "High Score: iteration 3948, score=0.79507\n",
      "High Score: iteration 3964, score=0.79521\n",
      "High Score: iteration 3965, score=0.79521\n",
      "High Score: iteration 3966, score=0.79527\n",
      "High Score: iteration 3992, score=0.79530\n",
      "High Score: iteration 3993, score=0.79530\n",
      "[4000]\ttraining's binary_logloss: 0.182832\ttraining's amex_metric: 0.860139\tvalid_1's binary_logloss: 0.217354\tvalid_1's amex_metric: 0.795035\n",
      "iteration 4000, score= 0.79497\n",
      "High Score: iteration 4080, score=0.79532\n",
      "High Score: iteration 4082, score=0.79532\n",
      "High Score: iteration 4088, score=0.79535\n",
      "High Score: iteration 4089, score=0.79535\n",
      "High Score: iteration 4090, score=0.79543\n",
      "iteration 4100, score= 0.79524\n",
      "High Score: iteration 4102, score=0.79546\n",
      "High Score: iteration 4103, score=0.79548\n",
      "High Score: iteration 4132, score=0.79555\n",
      "High Score: iteration 4133, score=0.79557\n",
      "High Score: iteration 4167, score=0.79562\n",
      "High Score: iteration 4168, score=0.79562\n",
      "High Score: iteration 4186, score=0.79566\n",
      "High Score: iteration 4187, score=0.79572\n",
      "High Score: iteration 4188, score=0.79572\n",
      "High Score: iteration 4189, score=0.79572\n",
      "High Score: iteration 4190, score=0.79572\n",
      "iteration 4200, score= 0.79532\n",
      "High Score: iteration 4251, score=0.79572\n",
      "High Score: iteration 4257, score=0.79572\n",
      "High Score: iteration 4259, score=0.79574\n",
      "High Score: iteration 4261, score=0.79574\n",
      "High Score: iteration 4263, score=0.79583\n",
      "High Score: iteration 4264, score=0.79583\n",
      "High Score: iteration 4265, score=0.79583\n",
      "High Score: iteration 4266, score=0.79585\n",
      "High Score: iteration 4269, score=0.79595\n",
      "High Score: iteration 4270, score=0.79606\n",
      "High Score: iteration 4273, score=0.79608\n",
      "High Score: iteration 4290, score=0.79609\n",
      "High Score: iteration 4291, score=0.79609\n",
      "High Score: iteration 4292, score=0.79611\n",
      "High Score: iteration 4297, score=0.79611\n",
      "High Score: iteration 4298, score=0.79612\n",
      "iteration 4300, score= 0.79584\n",
      "High Score: iteration 4317, score=0.79615\n",
      "High Score: iteration 4321, score=0.79617\n",
      "High Score: iteration 4344, score=0.79624\n",
      "High Score: iteration 4345, score=0.79624\n",
      "High Score: iteration 4346, score=0.79624\n",
      "High Score: iteration 4347, score=0.79624\n",
      "High Score: iteration 4348, score=0.79626\n",
      "High Score: iteration 4350, score=0.79626\n",
      "High Score: iteration 4362, score=0.79627\n",
      "High Score: iteration 4371, score=0.79631\n",
      "High Score: iteration 4372, score=0.79632\n",
      "High Score: iteration 4373, score=0.79632\n",
      "High Score: iteration 4374, score=0.79632\n",
      "High Score: iteration 4397, score=0.79645\n",
      "High Score: iteration 4398, score=0.79645\n",
      "iteration 4400, score= 0.79639\n",
      "High Score: iteration 4401, score=0.79654\n",
      "High Score: iteration 4402, score=0.79654\n",
      "High Score: iteration 4424, score=0.79655\n",
      "High Score: iteration 4425, score=0.79657\n",
      "High Score: iteration 4433, score=0.79657\n",
      "High Score: iteration 4434, score=0.79657\n",
      "High Score: iteration 4435, score=0.79657\n",
      "High Score: iteration 4440, score=0.79660\n",
      "High Score: iteration 4441, score=0.79664\n",
      "High Score: iteration 4459, score=0.79665\n",
      "High Score: iteration 4460, score=0.79665\n",
      "High Score: iteration 4461, score=0.79665\n",
      "High Score: iteration 4465, score=0.79667\n",
      "High Score: iteration 4471, score=0.79667\n",
      "High Score: iteration 4472, score=0.79667\n",
      "High Score: iteration 4473, score=0.79667\n",
      "High Score: iteration 4474, score=0.79667\n",
      "High Score: iteration 4484, score=0.79678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Score: iteration 4485, score=0.79678\n",
      "High Score: iteration 4487, score=0.79679\n",
      "High Score: iteration 4488, score=0.79681\n",
      "High Score: iteration 4489, score=0.79681\n",
      "High Score: iteration 4494, score=0.79689\n",
      "High Score: iteration 4495, score=0.79689\n",
      "High Score: iteration 4497, score=0.79690\n",
      "[4500]\ttraining's binary_logloss: 0.177403\ttraining's amex_metric: 0.869575\tvalid_1's binary_logloss: 0.216605\tvalid_1's amex_metric: 0.796812\n",
      "iteration 4500, score= 0.79671\n",
      "High Score: iteration 4508, score=0.79690\n",
      "High Score: iteration 4509, score=0.79694\n",
      "High Score: iteration 4511, score=0.79694\n",
      "High Score: iteration 4513, score=0.79694\n",
      "iteration 4600, score= 0.79681\n",
      "iteration 4700, score= 0.79642\n",
      "iteration 4800, score= 0.79620\n",
      "iteration 4900, score= 0.79639\n",
      "[5000]\ttraining's binary_logloss: 0.172081\ttraining's amex_metric: 0.878253\tvalid_1's binary_logloss: 0.216004\tvalid_1's amex_metric: 0.796484\n",
      "iteration 5000, score= 0.79652\n",
      "iteration 5100, score= 0.79668\n",
      "iteration 5200, score= 0.79675\n",
      "High Score: iteration 5234, score=0.79697\n",
      "High Score: iteration 5262, score=0.79697\n",
      "High Score: iteration 5263, score=0.79697\n",
      "High Score: iteration 5264, score=0.79697\n",
      "High Score: iteration 5269, score=0.79697\n",
      "High Score: iteration 5270, score=0.79697\n",
      "High Score: iteration 5271, score=0.79699\n",
      "High Score: iteration 5272, score=0.79701\n",
      "High Score: iteration 5273, score=0.79701\n",
      "High Score: iteration 5287, score=0.79702\n",
      "High Score: iteration 5299, score=0.79704\n",
      "iteration 5300, score= 0.79704\n",
      "High Score: iteration 5300, score=0.79704\n",
      "High Score: iteration 5301, score=0.79704\n",
      "High Score: iteration 5303, score=0.79708\n",
      "High Score: iteration 5304, score=0.79710\n",
      "High Score: iteration 5305, score=0.79710\n",
      "High Score: iteration 5323, score=0.79719\n",
      "High Score: iteration 5364, score=0.79722\n",
      "High Score: iteration 5365, score=0.79724\n",
      "High Score: iteration 5367, score=0.79724\n",
      "High Score: iteration 5381, score=0.79724\n",
      "iteration 5400, score= 0.79697\n",
      "High Score: iteration 5466, score=0.79726\n",
      "High Score: iteration 5467, score=0.79726\n",
      "High Score: iteration 5471, score=0.79730\n",
      "High Score: iteration 5472, score=0.79730\n",
      "High Score: iteration 5473, score=0.79730\n",
      "High Score: iteration 5475, score=0.79732\n",
      "High Score: iteration 5485, score=0.79735\n",
      "High Score: iteration 5486, score=0.79735\n",
      "[5500]\ttraining's binary_logloss: 0.167309\ttraining's amex_metric: 0.886648\tvalid_1's binary_logloss: 0.215652\tvalid_1's amex_metric: 0.797184\n",
      "iteration 5500, score= 0.79719\n",
      "High Score: iteration 5502, score=0.79746\n",
      "High Score: iteration 5503, score=0.79748\n",
      "High Score: iteration 5504, score=0.79748\n",
      "iteration 5600, score= 0.79720\n",
      "High Score: iteration 5610, score=0.79749\n",
      "High Score: iteration 5612, score=0.79749\n",
      "High Score: iteration 5653, score=0.79752\n",
      "iteration 5700, score= 0.79705\n",
      "iteration 5800, score= 0.79690\n",
      "iteration 5900, score= 0.79730\n",
      "High Score: iteration 5902, score=0.79757\n",
      "High Score: iteration 5903, score=0.79757\n",
      "High Score: iteration 5904, score=0.79757\n",
      "High Score: iteration 5905, score=0.79757\n",
      "[6000]\ttraining's binary_logloss: 0.163223\ttraining's amex_metric: 0.894273\tvalid_1's binary_logloss: 0.215442\tvalid_1's amex_metric: 0.797317\n",
      "iteration 6000, score= 0.79727\n",
      "High Score: iteration 6092, score=0.79758\n",
      "High Score: iteration 6093, score=0.79758\n",
      "High Score: iteration 6094, score=0.79758\n",
      "High Score: iteration 6095, score=0.79760\n",
      "High Score: iteration 6096, score=0.79760\n",
      "High Score: iteration 6097, score=0.79760\n",
      "iteration 6100, score= 0.79739\n",
      "iteration 6200, score= 0.79647\n",
      "iteration 6300, score= 0.79701\n",
      "iteration 6400, score= 0.79696\n",
      "[6500]\ttraining's binary_logloss: 0.158921\ttraining's amex_metric: 0.901104\tvalid_1's binary_logloss: 0.215192\tvalid_1's amex_metric: 0.797135\n",
      "iteration 6500, score= 0.79714\n",
      "iteration 6600, score= 0.79715\n",
      "iteration 6700, score= 0.79667\n",
      "iteration 6800, score= 0.79699\n",
      "iteration 6900, score= 0.79717\n",
      "[7000]\ttraining's binary_logloss: 0.153995\ttraining's amex_metric: 0.908538\tvalid_1's binary_logloss: 0.2149\tvalid_1's amex_metric: 0.797224\n",
      "iteration 7000, score= 0.79712\n",
      "iteration 7100, score= 0.79701\n",
      "iteration 7200, score= 0.79745\n",
      "iteration 7300, score= 0.79761\n",
      "High Score: iteration 7300, score=0.79761\n",
      "High Score: iteration 7323, score=0.79761\n",
      "High Score: iteration 7324, score=0.79763\n",
      "High Score: iteration 7326, score=0.79763\n",
      "High Score: iteration 7328, score=0.79763\n",
      "High Score: iteration 7330, score=0.79763\n",
      "High Score: iteration 7343, score=0.79765\n",
      "High Score: iteration 7349, score=0.79766\n",
      "High Score: iteration 7359, score=0.79766\n",
      "High Score: iteration 7360, score=0.79766\n",
      "High Score: iteration 7361, score=0.79766\n",
      "High Score: iteration 7362, score=0.79766\n",
      "High Score: iteration 7366, score=0.79766\n",
      "High Score: iteration 7369, score=0.79772\n",
      "High Score: iteration 7370, score=0.79772\n",
      "High Score: iteration 7371, score=0.79772\n",
      "High Score: iteration 7373, score=0.79774\n",
      "High Score: iteration 7396, score=0.79782\n",
      "High Score: iteration 7397, score=0.79787\n",
      "iteration 7400, score= 0.79780\n",
      "[7500]\ttraining's binary_logloss: 0.149308\ttraining's amex_metric: 0.916428\tvalid_1's binary_logloss: 0.214725\tvalid_1's amex_metric: 0.797392\n",
      "iteration 7500, score= 0.79746\n",
      "iteration 7600, score= 0.79727\n",
      "iteration 7700, score= 0.79725\n",
      "High Score: iteration 7775, score=0.79788\n",
      "High Score: iteration 7776, score=0.79791\n",
      "High Score: iteration 7777, score=0.79792\n",
      "High Score: iteration 7778, score=0.79793\n",
      "iteration 7800, score= 0.79767\n",
      "High Score: iteration 7835, score=0.79794\n",
      "High Score: iteration 7873, score=0.79805\n",
      "iteration 7900, score= 0.79763\n",
      "[8000]\ttraining's binary_logloss: 0.145136\ttraining's amex_metric: 0.923218\tvalid_1's binary_logloss: 0.214669\tvalid_1's amex_metric: 0.797131\n",
      "iteration 8000, score= 0.79713\n",
      "iteration 8100, score= 0.79778\n",
      "iteration 8200, score= 0.79740\n",
      "iteration 8300, score= 0.79752\n",
      "iteration 8400, score= 0.79769\n",
      "[8500]\ttraining's binary_logloss: 0.141555\ttraining's amex_metric: 0.929723\tvalid_1's binary_logloss: 0.214572\tvalid_1's amex_metric: 0.797816\n",
      "iteration 8500, score= 0.79780\n",
      "High Score: iteration 8526, score=0.79809\n",
      "High Score: iteration 8537, score=0.79811\n",
      "High Score: iteration 8543, score=0.79813\n",
      "iteration 8600, score= 0.79778\n",
      "High Score: iteration 8641, score=0.79814\n",
      "High Score: iteration 8642, score=0.79814\n",
      "High Score: iteration 8643, score=0.79814\n",
      "High Score: iteration 8668, score=0.79822\n",
      "High Score: iteration 8669, score=0.79839\n",
      "iteration 8700, score= 0.79814\n",
      "iteration 8800, score= 0.79813\n",
      "iteration 8900, score= 0.79774\n",
      "[9000]\ttraining's binary_logloss: 0.137528\ttraining's amex_metric: 0.935649\tvalid_1's binary_logloss: 0.214461\tvalid_1's amex_metric: 0.797803\n",
      "iteration 9000, score= 0.79780\n",
      "iteration 9100, score= 0.79748\n",
      "iteration 9200, score= 0.79763\n",
      "iteration 9300, score= 0.79785\n",
      "iteration 9400, score= 0.79789\n",
      "[9500]\ttraining's binary_logloss: 0.133914\ttraining's amex_metric: 0.941223\tvalid_1's binary_logloss: 0.214424\tvalid_1's amex_metric: 0.797935\n",
      "iteration 9500, score= 0.79796\n",
      "iteration 9600, score= 0.79800\n",
      "iteration 9700, score= 0.79784\n",
      "iteration 9800, score= 0.79774\n",
      "iteration 9900, score= 0.79790\n",
      "[10000]\ttraining's binary_logloss: 0.130352\ttraining's amex_metric: 0.946668\tvalid_1's binary_logloss: 0.214392\tvalid_1's amex_metric: 0.797813\n",
      "iteration 10000, score= 0.79779\n",
      "iteration 10100, score= 0.79782\n",
      "iteration 10200, score= 0.79750\n",
      "iteration 10300, score= 0.79770\n",
      "iteration 10400, score= 0.79763\n",
      "[10500]\ttraining's binary_logloss: 0.127231\ttraining's amex_metric: 0.95114\tvalid_1's binary_logloss: 0.214339\tvalid_1's amex_metric: 0.797538\n",
      "iteration 10500, score= 0.79758\n",
      "iteration 10600, score= 0.79718\n",
      "iteration 10700, score= 0.79727\n",
      "iteration 10800, score= 0.79778\n",
      "iteration 10900, score= 0.79805\n",
      "High Score: iteration 10964, score=0.79840\n",
      "[11000]\ttraining's binary_logloss: 0.124062\ttraining's amex_metric: 0.955065\tvalid_1's binary_logloss: 0.214294\tvalid_1's amex_metric: 0.798187\n",
      "iteration 11000, score= 0.79819\n",
      "iteration 11100, score= 0.79797\n",
      "iteration 11200, score= 0.79800\n",
      "iteration 11300, score= 0.79756\n",
      "iteration 11400, score= 0.79739\n",
      "[11500]\ttraining's binary_logloss: 0.120741\ttraining's amex_metric: 0.959508\tvalid_1's binary_logloss: 0.214298\tvalid_1's amex_metric: 0.797183\n",
      "iteration 11500, score= 0.79731\n",
      "iteration 11600, score= 0.79737\n",
      "iteration 11700, score= 0.79750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11800, score= 0.79711\n",
      "iteration 11900, score= 0.79728\n",
      "[12000]\ttraining's binary_logloss: 0.117741\ttraining's amex_metric: 0.963447\tvalid_1's binary_logloss: 0.214302\tvalid_1's amex_metric: 0.797759\n",
      "iteration 12000, score= 0.79776\n",
      "iteration 12100, score= 0.79770\n",
      "iteration 12200, score= 0.79786\n",
      "iteration 12300, score= 0.79777\n",
      "iteration 12400, score= 0.79780\n",
      "[12500]\ttraining's binary_logloss: 0.115167\ttraining's amex_metric: 0.966702\tvalid_1's binary_logloss: 0.21433\tvalid_1's amex_metric: 0.797589\n",
      "iteration 12500, score= 0.79776\n",
      "iteration 12600, score= 0.79736\n",
      "iteration 12700, score= 0.79759\n",
      "iteration 12800, score= 0.79751\n",
      "iteration 12900, score= 0.79746\n",
      "[13000]\ttraining's binary_logloss: 0.111838\ttraining's amex_metric: 0.97049\tvalid_1's binary_logloss: 0.214402\tvalid_1's amex_metric: 0.796972\n",
      "iteration 13000, score= 0.79697\n",
      "iteration 13100, score= 0.79706\n",
      "iteration 13200, score= 0.79755\n",
      "iteration 13300, score= 0.79780\n",
      "iteration 13400, score= 0.79752\n",
      "[13500]\ttraining's binary_logloss: 0.109178\ttraining's amex_metric: 0.973847\tvalid_1's binary_logloss: 0.214444\tvalid_1's amex_metric: 0.797788\n",
      "iteration 13500, score= 0.79779\n",
      "iteration 13600, score= 0.79775\n",
      "iteration 13700, score= 0.79741\n",
      "iteration 13800, score= 0.79762\n",
      "iteration 13900, score= 0.79782\n",
      "[14000]\ttraining's binary_logloss: 0.106183\ttraining's amex_metric: 0.977049\tvalid_1's binary_logloss: 0.214486\tvalid_1's amex_metric: 0.798091\n",
      "iteration 14000, score= 0.79811\n",
      "iteration 14100, score= 0.79780\n",
      "iteration 14200, score= 0.79750\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50)\n",
    "print(\"Training fold {} with {} features...\".format(target_fold, len(features)))\n",
    "\n",
    "global max_score \n",
    "max_score = 0.793\n",
    "\n",
    "def save_model():\n",
    "    def callback(env):\n",
    "        global max_score\n",
    "        iteration = env.iteration\n",
    "        score = env.evaluation_result_list[3][2]\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"iteration {}, score= {:.05f}\".format(iteration,score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            print(\"High Score: iteration {}, score={:.05f}\".format(iteration, score))\n",
    "            dump(env.model, os.path.join(save_path, \"{:.05f}.pkl\".format(score)))\n",
    "\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=20000,\n",
    "    valid_sets = [lgb_train, lgb_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval = 500,\n",
    "    feval = lgb_amex_metric,\n",
    "    callbacks=[save_model()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training(train):\n",
    "    \n",
    "#     # create a numpy array to store out of folds predictions\n",
    "#     oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "#     kfold = StratifiedKFold(\n",
    "#         n_splits=n_folds, \n",
    "#         shuffle=True, \n",
    "#         random_state=seed\n",
    "#     )\n",
    "    \n",
    "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "#         print(\"#\" * 50)\n",
    "#         print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "#         x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "#         y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "        \n",
    "#         lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "#         lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "#         model = lgb.train(\n",
    "#             params=params,\n",
    "#             train_set=lgb_train,\n",
    "#             num_boost_round=10500,\n",
    "#             valid_sets = [lgb_train, lgb_valid],\n",
    "#             early_stopping_rounds=100,\n",
    "#             verbose_eval = 500,\n",
    "#             feval = lgb_amex_metric\n",
    "#         )\n",
    "#         # save best model\n",
    "#         dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "#         # predict validation\n",
    "#         val_pred = model.predict(x_val)\n",
    "        \n",
    "#         # add to out of folds array\n",
    "#         oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "#         # compute fold metric\n",
    "#         score = amex_metric(y_val, val_pred)\n",
    "#         print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "#         del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "#         gc.collect()\n",
    "        \n",
    "#     # compute oof\n",
    "#     score = amex_metric(train[target], oof_predictions)\n",
    "#     print(\"oof score is {}\".format(score))\n",
    "    \n",
    "#     # create a dataframe to store out of folds predictions\n",
    "#     oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "#     oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "#     return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
