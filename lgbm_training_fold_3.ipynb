{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_full_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = load(\"selected_features.pkl\")\n",
    "\n",
    "target = \"target\"\n",
    "\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "cat_features = [feature for feature in cat_features if feature in features]\n",
    "            \n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fold = 3\n",
    "\n",
    "kfold = StratifiedKFold(\n",
    "    n_splits=n_folds, \n",
    "    shuffle=True, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "    \n",
    "    if fold == target_fold:\n",
    "        break\n",
    "\n",
    "x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, x_train, x_val, y_train, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(\"../ckpt/lgbm_seed_{}\".format(seed))\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "save_path = os.path.join(save_folder, \"fold_{}\".format(target_fold))\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 3 with 1950 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.555485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211969\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "iteration 0, score= 0.69032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100, score= 0.75739\n",
      "iteration 200, score= 0.75829\n",
      "iteration 300, score= 0.76111\n",
      "iteration 400, score= 0.76257\n",
      "[500]\ttraining's binary_logloss: 0.337087\ttraining's amex_metric: 0.778135\tvalid_1's binary_logloss: 0.341015\tvalid_1's amex_metric: 0.765914\n",
      "iteration 500, score= 0.76617\n",
      "iteration 600, score= 0.76798\n",
      "iteration 700, score= 0.77036\n",
      "iteration 800, score= 0.77311\n",
      "iteration 900, score= 0.77610\n",
      "[1000]\ttraining's binary_logloss: 0.246021\ttraining's amex_metric: 0.795077\tvalid_1's binary_logloss: 0.25445\tvalid_1's amex_metric: 0.777348\n",
      "iteration 1000, score= 0.77735\n",
      "iteration 1100, score= 0.77965\n",
      "iteration 1200, score= 0.78094\n",
      "iteration 1300, score= 0.78244\n",
      "iteration 1400, score= 0.78332\n",
      "[1500]\ttraining's binary_logloss: 0.221716\ttraining's amex_metric: 0.808706\tvalid_1's binary_logloss: 0.234523\tvalid_1's amex_metric: 0.783853\n",
      "iteration 1500, score= 0.78373\n",
      "iteration 1600, score= 0.78497\n",
      "iteration 1700, score= 0.78619\n",
      "iteration 1800, score= 0.78633\n",
      "iteration 1900, score= 0.78678\n",
      "[2000]\ttraining's binary_logloss: 0.208175\ttraining's amex_metric: 0.821401\tvalid_1's binary_logloss: 0.225967\tvalid_1's amex_metric: 0.787986\n",
      "iteration 2000, score= 0.78793\n",
      "iteration 2100, score= 0.78842\n",
      "iteration 2200, score= 0.78878\n",
      "iteration 2300, score= 0.78936\n",
      "iteration 2400, score= 0.78955\n",
      "[2500]\ttraining's binary_logloss: 0.201316\ttraining's amex_metric: 0.831502\tvalid_1's binary_logloss: 0.223119\tvalid_1's amex_metric: 0.7905\n",
      "iteration 2500, score= 0.79048\n",
      "iteration 2600, score= 0.79129\n",
      "iteration 2700, score= 0.79114\n",
      "iteration 2800, score= 0.79117\n",
      "iteration 2900, score= 0.79110\n",
      "[3000]\ttraining's binary_logloss: 0.194445\ttraining's amex_metric: 0.840922\tvalid_1's binary_logloss: 0.220789\tvalid_1's amex_metric: 0.790955\n",
      "iteration 3000, score= 0.79096\n",
      "iteration 3100, score= 0.79141\n",
      "iteration 3200, score= 0.79221\n",
      "iteration 3300, score= 0.79248\n",
      "iteration 3400, score= 0.79251\n",
      "High Score: iteration 3452, score=0.79308\n",
      "High Score: iteration 3453, score=0.79311\n",
      "High Score: iteration 3468, score=0.79314\n",
      "High Score: iteration 3469, score=0.79314\n",
      "High Score: iteration 3470, score=0.79314\n",
      "High Score: iteration 3474, score=0.79318\n",
      "High Score: iteration 3475, score=0.79325\n",
      "High Score: iteration 3476, score=0.79325\n",
      "High Score: iteration 3478, score=0.79325\n",
      "High Score: iteration 3486, score=0.79332\n",
      "High Score: iteration 3490, score=0.79334\n",
      "High Score: iteration 3491, score=0.79334\n",
      "[3500]\ttraining's binary_logloss: 0.188163\ttraining's amex_metric: 0.850568\tvalid_1's binary_logloss: 0.219227\tvalid_1's amex_metric: 0.793181\n",
      "iteration 3500, score= 0.79324\n",
      "High Score: iteration 3519, score=0.79336\n",
      "High Score: iteration 3520, score=0.79336\n",
      "High Score: iteration 3521, score=0.79336\n",
      "High Score: iteration 3525, score=0.79343\n",
      "High Score: iteration 3526, score=0.79345\n",
      "High Score: iteration 3542, score=0.79346\n",
      "High Score: iteration 3543, score=0.79359\n",
      "iteration 3600, score= 0.79350\n",
      "High Score: iteration 3647, score=0.79373\n",
      "High Score: iteration 3665, score=0.79374\n",
      "High Score: iteration 3666, score=0.79381\n",
      "High Score: iteration 3667, score=0.79381\n",
      "High Score: iteration 3673, score=0.79383\n",
      "High Score: iteration 3674, score=0.79387\n",
      "High Score: iteration 3675, score=0.79387\n",
      "High Score: iteration 3685, score=0.79388\n",
      "High Score: iteration 3694, score=0.79395\n",
      "High Score: iteration 3695, score=0.79397\n",
      "iteration 3700, score= 0.79355\n",
      "High Score: iteration 3747, score=0.79400\n",
      "High Score: iteration 3750, score=0.79400\n",
      "High Score: iteration 3751, score=0.79415\n",
      "High Score: iteration 3754, score=0.79421\n",
      "High Score: iteration 3755, score=0.79423\n",
      "High Score: iteration 3756, score=0.79423\n",
      "High Score: iteration 3757, score=0.79425\n",
      "High Score: iteration 3758, score=0.79428\n",
      "High Score: iteration 3764, score=0.79430\n",
      "High Score: iteration 3766, score=0.79434\n",
      "High Score: iteration 3767, score=0.79434\n",
      "High Score: iteration 3768, score=0.79437\n",
      "High Score: iteration 3773, score=0.79439\n",
      "High Score: iteration 3774, score=0.79439\n",
      "High Score: iteration 3777, score=0.79443\n",
      "iteration 3800, score= 0.79398\n",
      "High Score: iteration 3814, score=0.79446\n",
      "High Score: iteration 3842, score=0.79453\n",
      "High Score: iteration 3844, score=0.79460\n",
      "High Score: iteration 3845, score=0.79460\n",
      "High Score: iteration 3875, score=0.79461\n",
      "High Score: iteration 3876, score=0.79461\n",
      "High Score: iteration 3877, score=0.79463\n",
      "High Score: iteration 3883, score=0.79470\n",
      "High Score: iteration 3884, score=0.79470\n",
      "High Score: iteration 3885, score=0.79470\n",
      "High Score: iteration 3886, score=0.79472\n",
      "High Score: iteration 3887, score=0.79472\n",
      "High Score: iteration 3888, score=0.79472\n",
      "iteration 3900, score= 0.79413\n",
      "High Score: iteration 3978, score=0.79475\n",
      "High Score: iteration 3996, score=0.79486\n",
      "[4000]\ttraining's binary_logloss: 0.182649\ttraining's amex_metric: 0.860017\tvalid_1's binary_logloss: 0.218329\tvalid_1's amex_metric: 0.794836\n",
      "iteration 4000, score= 0.79482\n",
      "High Score: iteration 4011, score=0.79487\n",
      "High Score: iteration 4013, score=0.79489\n",
      "High Score: iteration 4014, score=0.79489\n",
      "High Score: iteration 4015, score=0.79493\n",
      "High Score: iteration 4016, score=0.79493\n",
      "High Score: iteration 4017, score=0.79493\n",
      "High Score: iteration 4018, score=0.79493\n",
      "High Score: iteration 4027, score=0.79496\n",
      "High Score: iteration 4029, score=0.79500\n",
      "High Score: iteration 4048, score=0.79507\n",
      "High Score: iteration 4049, score=0.79507\n",
      "High Score: iteration 4050, score=0.79507\n",
      "High Score: iteration 4055, score=0.79511\n",
      "High Score: iteration 4056, score=0.79511\n",
      "iteration 4100, score= 0.79497\n",
      "High Score: iteration 4105, score=0.79521\n",
      "High Score: iteration 4106, score=0.79531\n",
      "High Score: iteration 4179, score=0.79533\n",
      "High Score: iteration 4194, score=0.79538\n",
      "High Score: iteration 4197, score=0.79544\n",
      "High Score: iteration 4198, score=0.79546\n",
      "iteration 4200, score= 0.79544\n",
      "High Score: iteration 4215, score=0.79553\n",
      "High Score: iteration 4217, score=0.79553\n",
      "High Score: iteration 4218, score=0.79553\n",
      "High Score: iteration 4220, score=0.79553\n",
      "High Score: iteration 4221, score=0.79557\n",
      "High Score: iteration 4238, score=0.79560\n",
      "High Score: iteration 4239, score=0.79560\n",
      "High Score: iteration 4240, score=0.79565\n",
      "High Score: iteration 4241, score=0.79565\n",
      "High Score: iteration 4248, score=0.79569\n",
      "High Score: iteration 4249, score=0.79569\n",
      "High Score: iteration 4250, score=0.79569\n",
      "iteration 4300, score= 0.79546\n",
      "High Score: iteration 4321, score=0.79574\n",
      "High Score: iteration 4339, score=0.79575\n",
      "High Score: iteration 4340, score=0.79575\n",
      "High Score: iteration 4342, score=0.79577\n",
      "High Score: iteration 4377, score=0.79579\n",
      "High Score: iteration 4378, score=0.79581\n",
      "High Score: iteration 4379, score=0.79583\n",
      "High Score: iteration 4381, score=0.79583\n",
      "High Score: iteration 4387, score=0.79584\n",
      "High Score: iteration 4390, score=0.79594\n",
      "High Score: iteration 4391, score=0.79594\n",
      "iteration 4400, score= 0.79572\n",
      "High Score: iteration 4407, score=0.79596\n",
      "High Score: iteration 4408, score=0.79602\n",
      "High Score: iteration 4412, score=0.79602\n",
      "High Score: iteration 4488, score=0.79603\n",
      "High Score: iteration 4489, score=0.79603\n",
      "High Score: iteration 4493, score=0.79612\n",
      "[4500]\ttraining's binary_logloss: 0.177246\ttraining's amex_metric: 0.869166\tvalid_1's binary_logloss: 0.21755\tvalid_1's amex_metric: 0.795829\n",
      "iteration 4500, score= 0.79579\n",
      "High Score: iteration 4530, score=0.79629\n",
      "iteration 4600, score= 0.79564\n",
      "iteration 4700, score= 0.79594\n",
      "iteration 4800, score= 0.79548\n",
      "iteration 4900, score= 0.79585\n",
      "[5000]\ttraining's binary_logloss: 0.17194\ttraining's amex_metric: 0.877959\tvalid_1's binary_logloss: 0.21688\tvalid_1's amex_metric: 0.795783\n",
      "iteration 5000, score= 0.79599\n",
      "High Score: iteration 5037, score=0.79632\n",
      "High Score: iteration 5038, score=0.79632\n",
      "High Score: iteration 5079, score=0.79644\n",
      "iteration 5100, score= 0.79606\n",
      "iteration 5200, score= 0.79590\n",
      "iteration 5300, score= 0.79592\n",
      "iteration 5400, score= 0.79581\n",
      "[5500]\ttraining's binary_logloss: 0.167224\ttraining's amex_metric: 0.886327\tvalid_1's binary_logloss: 0.216557\tvalid_1's amex_metric: 0.795797\n",
      "iteration 5500, score= 0.79580\n",
      "iteration 5600, score= 0.79575\n",
      "iteration 5700, score= 0.79565\n",
      "iteration 5800, score= 0.79583\n",
      "High Score: iteration 5893, score=0.79648\n",
      "High Score: iteration 5894, score=0.79655\n",
      "iteration 5900, score= 0.79646\n",
      "[6000]\ttraining's binary_logloss: 0.163119\ttraining's amex_metric: 0.893753\tvalid_1's binary_logloss: 0.216342\tvalid_1's amex_metric: 0.796198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6000, score= 0.79632\n",
      "iteration 6100, score= 0.79622\n",
      "High Score: iteration 6194, score=0.79663\n",
      "High Score: iteration 6195, score=0.79663\n",
      "iteration 6200, score= 0.79636\n",
      "iteration 6300, score= 0.79614\n",
      "iteration 6400, score= 0.79627\n",
      "High Score: iteration 6489, score=0.79666\n",
      "High Score: iteration 6490, score=0.79666\n",
      "High Score: iteration 6491, score=0.79669\n",
      "High Score: iteration 6492, score=0.79673\n",
      "[6500]\ttraining's binary_logloss: 0.158833\ttraining's amex_metric: 0.901121\tvalid_1's binary_logloss: 0.216108\tvalid_1's amex_metric: 0.796456\n",
      "iteration 6500, score= 0.79648\n",
      "High Score: iteration 6521, score=0.79680\n",
      "iteration 6600, score= 0.79653\n",
      "High Score: iteration 6693, score=0.79682\n",
      "High Score: iteration 6694, score=0.79682\n",
      "High Score: iteration 6695, score=0.79684\n",
      "High Score: iteration 6696, score=0.79686\n",
      "High Score: iteration 6697, score=0.79686\n",
      "iteration 6700, score= 0.79678\n",
      "High Score: iteration 6713, score=0.79687\n",
      "High Score: iteration 6715, score=0.79687\n",
      "High Score: iteration 6716, score=0.79689\n",
      "High Score: iteration 6729, score=0.79689\n",
      "High Score: iteration 6730, score=0.79693\n",
      "High Score: iteration 6731, score=0.79693\n",
      "High Score: iteration 6732, score=0.79693\n",
      "High Score: iteration 6733, score=0.79710\n",
      "High Score: iteration 6734, score=0.79710\n",
      "High Score: iteration 6735, score=0.79725\n",
      "High Score: iteration 6745, score=0.79725\n",
      "High Score: iteration 6746, score=0.79725\n",
      "High Score: iteration 6747, score=0.79725\n",
      "High Score: iteration 6756, score=0.79725\n",
      "High Score: iteration 6757, score=0.79725\n",
      "High Score: iteration 6758, score=0.79725\n",
      "High Score: iteration 6759, score=0.79751\n",
      "High Score: iteration 6760, score=0.79759\n",
      "iteration 6800, score= 0.79728\n",
      "iteration 6900, score= 0.79746\n",
      "[7000]\ttraining's binary_logloss: 0.153853\ttraining's amex_metric: 0.909123\tvalid_1's binary_logloss: 0.215823\tvalid_1's amex_metric: 0.796859\n",
      "iteration 7000, score= 0.79692\n",
      "High Score: iteration 7080, score=0.79766\n",
      "iteration 7100, score= 0.79751\n",
      "High Score: iteration 7113, score=0.79771\n",
      "High Score: iteration 7118, score=0.79771\n",
      "High Score: iteration 7119, score=0.79771\n",
      "High Score: iteration 7120, score=0.79779\n",
      "High Score: iteration 7121, score=0.79779\n",
      "iteration 7200, score= 0.79746\n",
      "iteration 7300, score= 0.79732\n",
      "iteration 7400, score= 0.79711\n",
      "High Score: iteration 7479, score=0.79785\n",
      "High Score: iteration 7480, score=0.79785\n",
      "High Score: iteration 7481, score=0.79791\n",
      "High Score: iteration 7482, score=0.79796\n",
      "[7500]\ttraining's binary_logloss: 0.149148\ttraining's amex_metric: 0.91661\tvalid_1's binary_logloss: 0.215628\tvalid_1's amex_metric: 0.797603\n",
      "iteration 7500, score= 0.79756\n",
      "High Score: iteration 7557, score=0.79797\n",
      "High Score: iteration 7573, score=0.79805\n",
      "High Score: iteration 7574, score=0.79816\n",
      "High Score: iteration 7575, score=0.79816\n",
      "High Score: iteration 7588, score=0.79818\n",
      "iteration 7600, score= 0.79801\n",
      "iteration 7700, score= 0.79775\n",
      "iteration 7800, score= 0.79757\n",
      "iteration 7900, score= 0.79752\n",
      "[8000]\ttraining's binary_logloss: 0.14498\ttraining's amex_metric: 0.923806\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex_metric: 0.797412\n",
      "iteration 8000, score= 0.79737\n",
      "iteration 8100, score= 0.79779\n",
      "High Score: iteration 8107, score=0.79819\n",
      "High Score: iteration 8108, score=0.79819\n",
      "High Score: iteration 8109, score=0.79822\n",
      "High Score: iteration 8110, score=0.79822\n",
      "High Score: iteration 8111, score=0.79824\n",
      "High Score: iteration 8112, score=0.79826\n",
      "High Score: iteration 8126, score=0.79830\n",
      "iteration 8200, score= 0.79753\n",
      "iteration 8300, score= 0.79723\n",
      "iteration 8400, score= 0.79725\n",
      "[8500]\ttraining's binary_logloss: 0.141404\ttraining's amex_metric: 0.929791\tvalid_1's binary_logloss: 0.215371\tvalid_1's amex_metric: 0.797557\n",
      "iteration 8500, score= 0.79758\n",
      "iteration 8600, score= 0.79712\n",
      "iteration 8700, score= 0.79710\n",
      "iteration 8800, score= 0.79724\n",
      "iteration 8900, score= 0.79731\n",
      "[9000]\ttraining's binary_logloss: 0.137385\ttraining's amex_metric: 0.935982\tvalid_1's binary_logloss: 0.215252\tvalid_1's amex_metric: 0.79761\n",
      "iteration 9000, score= 0.79757\n",
      "iteration 9100, score= 0.79767\n",
      "iteration 9200, score= 0.79747\n",
      "iteration 9300, score= 0.79727\n",
      "iteration 9400, score= 0.79714\n",
      "[9500]\ttraining's binary_logloss: 0.133748\ttraining's amex_metric: 0.941502\tvalid_1's binary_logloss: 0.215188\tvalid_1's amex_metric: 0.797104\n",
      "iteration 9500, score= 0.79708\n",
      "iteration 9600, score= 0.79736\n",
      "iteration 9700, score= 0.79747\n",
      "iteration 9800, score= 0.79737\n",
      "iteration 9900, score= 0.79749\n",
      "[10000]\ttraining's binary_logloss: 0.130179\ttraining's amex_metric: 0.946749\tvalid_1's binary_logloss: 0.215108\tvalid_1's amex_metric: 0.797429\n",
      "iteration 10000, score= 0.79739\n",
      "iteration 10100, score= 0.79730\n",
      "iteration 10200, score= 0.79736\n",
      "iteration 10300, score= 0.79774\n",
      "iteration 10400, score= 0.79779\n",
      "[10500]\ttraining's binary_logloss: 0.127098\ttraining's amex_metric: 0.951229\tvalid_1's binary_logloss: 0.2151\tvalid_1's amex_metric: 0.797179\n",
      "iteration 10500, score= 0.79714\n",
      "iteration 10600, score= 0.79713\n",
      "iteration 10700, score= 0.79715\n",
      "iteration 10800, score= 0.79743\n",
      "iteration 10900, score= 0.79717\n",
      "[11000]\ttraining's binary_logloss: 0.123904\ttraining's amex_metric: 0.955709\tvalid_1's binary_logloss: 0.215079\tvalid_1's amex_metric: 0.797017\n",
      "iteration 11000, score= 0.79698\n",
      "iteration 11100, score= 0.79679\n",
      "iteration 11200, score= 0.79703\n",
      "iteration 11300, score= 0.79700\n",
      "iteration 11400, score= 0.79716\n",
      "[11500]\ttraining's binary_logloss: 0.120602\ttraining's amex_metric: 0.960186\tvalid_1's binary_logloss: 0.215096\tvalid_1's amex_metric: 0.796889\n",
      "iteration 11500, score= 0.79680\n",
      "iteration 11600, score= 0.79708\n",
      "iteration 11700, score= 0.79723\n",
      "iteration 11800, score= 0.79725\n",
      "iteration 11900, score= 0.79765\n",
      "[12000]\ttraining's binary_logloss: 0.117604\ttraining's amex_metric: 0.964113\tvalid_1's binary_logloss: 0.215103\tvalid_1's amex_metric: 0.798203\n",
      "iteration 12000, score= 0.79818\n",
      "High Score: iteration 12035, score=0.79831\n",
      "High Score: iteration 12039, score=0.79835\n",
      "High Score: iteration 12040, score=0.79837\n",
      "High Score: iteration 12041, score=0.79839\n",
      "High Score: iteration 12042, score=0.79839\n",
      "High Score: iteration 12043, score=0.79839\n",
      "High Score: iteration 12047, score=0.79844\n",
      "iteration 12100, score= 0.79794\n",
      "iteration 12200, score= 0.79787\n",
      "iteration 12300, score= 0.79794\n",
      "iteration 12400, score= 0.79798\n",
      "[12500]\ttraining's binary_logloss: 0.114978\ttraining's amex_metric: 0.967339\tvalid_1's binary_logloss: 0.21509\tvalid_1's amex_metric: 0.798111\n",
      "iteration 12500, score= 0.79807\n",
      "iteration 12600, score= 0.79796\n",
      "iteration 12700, score= 0.79812\n",
      "High Score: iteration 12765, score=0.79844\n",
      "High Score: iteration 12782, score=0.79844\n",
      "iteration 12800, score= 0.79832\n",
      "iteration 12900, score= 0.79827\n",
      "[13000]\ttraining's binary_logloss: 0.111631\ttraining's amex_metric: 0.971153\tvalid_1's binary_logloss: 0.215161\tvalid_1's amex_metric: 0.798299\n",
      "iteration 13000, score= 0.79824\n",
      "High Score: iteration 13092, score=0.79849\n",
      "iteration 13100, score= 0.79832\n",
      "High Score: iteration 13107, score=0.79857\n",
      "High Score: iteration 13112, score=0.79857\n",
      "High Score: iteration 13113, score=0.79859\n",
      "High Score: iteration 13114, score=0.79864\n",
      "High Score: iteration 13137, score=0.79868\n",
      "High Score: iteration 13139, score=0.79870\n",
      "High Score: iteration 13156, score=0.79877\n",
      "High Score: iteration 13178, score=0.79879\n",
      "High Score: iteration 13179, score=0.79881\n",
      "iteration 13200, score= 0.79866\n",
      "iteration 13300, score= 0.79837\n",
      "iteration 13400, score= 0.79837\n",
      "[13500]\ttraining's binary_logloss: 0.108947\ttraining's amex_metric: 0.974593\tvalid_1's binary_logloss: 0.215152\tvalid_1's amex_metric: 0.798647\n",
      "iteration 13500, score= 0.79865\n",
      "iteration 13600, score= 0.79844\n",
      "iteration 13700, score= 0.79819\n",
      "iteration 13800, score= 0.79822\n",
      "iteration 13900, score= 0.79814\n",
      "[14000]\ttraining's binary_logloss: 0.105981\ttraining's amex_metric: 0.977612\tvalid_1's binary_logloss: 0.215158\tvalid_1's amex_metric: 0.798227\n",
      "iteration 14000, score= 0.79823\n",
      "iteration 14100, score= 0.79840\n",
      "iteration 14200, score= 0.79846\n",
      "iteration 14300, score= 0.79826\n",
      "iteration 14400, score= 0.79860\n",
      "High Score: iteration 14437, score=0.79882\n",
      "High Score: iteration 14448, score=0.79882\n",
      "High Score: iteration 14449, score=0.79882\n",
      "[14500]\ttraining's binary_logloss: 0.103666\ttraining's amex_metric: 0.980217\tvalid_1's binary_logloss: 0.215189\tvalid_1's amex_metric: 0.798319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14500, score= 0.79832\n",
      "iteration 14600, score= 0.79786\n",
      "iteration 14700, score= 0.79752\n",
      "iteration 14800, score= 0.79801\n",
      "iteration 14900, score= 0.79805\n",
      "[15000]\ttraining's binary_logloss: 0.10132\ttraining's amex_metric: 0.982473\tvalid_1's binary_logloss: 0.215224\tvalid_1's amex_metric: 0.798408\n",
      "iteration 15000, score= 0.79824\n",
      "iteration 15100, score= 0.79820\n",
      "iteration 15200, score= 0.79828\n",
      "iteration 15300, score= 0.79798\n",
      "iteration 15400, score= 0.79831\n",
      "[15500]\ttraining's binary_logloss: 0.0988281\ttraining's amex_metric: 0.984617\tvalid_1's binary_logloss: 0.215322\tvalid_1's amex_metric: 0.798503\n",
      "iteration 15500, score= 0.79844\n",
      "iteration 15600, score= 0.79848\n",
      "iteration 15700, score= 0.79832\n",
      "High Score: iteration 15745, score=0.79888\n",
      "iteration 15800, score= 0.79846\n",
      "High Score: iteration 15861, score=0.79893\n",
      "High Score: iteration 15869, score=0.79893\n",
      "High Score: iteration 15896, score=0.79895\n",
      "High Score: iteration 15897, score=0.79904\n",
      "High Score: iteration 15899, score=0.79904\n",
      "iteration 15900, score= 0.79904\n",
      "High Score: iteration 15901, score=0.79908\n",
      "High Score: iteration 15920, score=0.79916\n",
      "[16000]\ttraining's binary_logloss: 0.0961994\ttraining's amex_metric: 0.986679\tvalid_1's binary_logloss: 0.215487\tvalid_1's amex_metric: 0.798395\n",
      "iteration 16000, score= 0.79839\n",
      "iteration 16100, score= 0.79844\n",
      "iteration 16200, score= 0.79838\n",
      "iteration 16300, score= 0.79848\n",
      "iteration 16400, score= 0.79852\n",
      "[16500]\ttraining's binary_logloss: 0.0935924\ttraining's amex_metric: 0.988653\tvalid_1's binary_logloss: 0.215596\tvalid_1's amex_metric: 0.798412\n",
      "iteration 16500, score= 0.79841\n",
      "iteration 16600, score= 0.79799\n",
      "iteration 16700, score= 0.79830\n",
      "iteration 16800, score= 0.79822\n",
      "iteration 16900, score= 0.79835\n",
      "[17000]\ttraining's binary_logloss: 0.0914537\ttraining's amex_metric: 0.990482\tvalid_1's binary_logloss: 0.215659\tvalid_1's amex_metric: 0.798453\n",
      "iteration 17000, score= 0.79841\n",
      "iteration 17100, score= 0.79851\n",
      "iteration 17200, score= 0.79908\n",
      "iteration 17300, score= 0.79853\n",
      "iteration 17400, score= 0.79833\n",
      "[17500]\ttraining's binary_logloss: 0.089146\ttraining's amex_metric: 0.992054\tvalid_1's binary_logloss: 0.215792\tvalid_1's amex_metric: 0.798107\n",
      "iteration 17500, score= 0.79809\n",
      "iteration 17600, score= 0.79802\n",
      "iteration 17700, score= 0.79810\n",
      "iteration 17800, score= 0.79809\n",
      "iteration 17900, score= 0.79846\n",
      "[18000]\ttraining's binary_logloss: 0.0871976\ttraining's amex_metric: 0.993179\tvalid_1's binary_logloss: 0.21592\tvalid_1's amex_metric: 0.798681\n",
      "iteration 18000, score= 0.79868\n",
      "iteration 18100, score= 0.79868\n",
      "iteration 18200, score= 0.79841\n",
      "iteration 18300, score= 0.79874\n",
      "iteration 18400, score= 0.79838\n",
      "[18500]\ttraining's binary_logloss: 0.0852222\ttraining's amex_metric: 0.994223\tvalid_1's binary_logloss: 0.216057\tvalid_1's amex_metric: 0.798334\n",
      "iteration 18500, score= 0.79833\n",
      "iteration 18600, score= 0.79857\n",
      "iteration 18700, score= 0.79833\n",
      "iteration 18800, score= 0.79814\n",
      "iteration 18900, score= 0.79859\n",
      "[19000]\ttraining's binary_logloss: 0.0831547\ttraining's amex_metric: 0.995075\tvalid_1's binary_logloss: 0.216211\tvalid_1's amex_metric: 0.798456\n",
      "iteration 19000, score= 0.79843\n",
      "iteration 19100, score= 0.79881\n",
      "iteration 19200, score= 0.79835\n",
      "iteration 19300, score= 0.79878\n",
      "iteration 19400, score= 0.79906\n",
      "[19500]\ttraining's binary_logloss: 0.080956\ttraining's amex_metric: 0.99597\tvalid_1's binary_logloss: 0.216406\tvalid_1's amex_metric: 0.798762\n",
      "iteration 19500, score= 0.79874\n",
      "iteration 19600, score= 0.79838\n",
      "iteration 19700, score= 0.79874\n",
      "iteration 19800, score= 0.79857\n",
      "iteration 19900, score= 0.79861\n",
      "[20000]\ttraining's binary_logloss: 0.079029\ttraining's amex_metric: 0.996695\tvalid_1's binary_logloss: 0.216576\tvalid_1's amex_metric: 0.798643\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50)\n",
    "print(\"Training fold {} with {} features...\".format(target_fold, len(features)))\n",
    "\n",
    "global max_score \n",
    "max_score = 0.793\n",
    "\n",
    "def save_model():\n",
    "    def callback(env):\n",
    "        global max_score\n",
    "        iteration = env.iteration\n",
    "        score = env.evaluation_result_list[3][2]\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"iteration {}, score= {:.05f}\".format(iteration,score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            print(\"High Score: iteration {}, score={:.05f}\".format(iteration, score))\n",
    "            dump(env.model, os.path.join(save_path, \"{:.05f}.pkl\".format(score)))\n",
    "\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=20000,\n",
    "    valid_sets = [lgb_train, lgb_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval = 500,\n",
    "    feval = lgb_amex_metric,\n",
    "    callbacks=[save_model()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training(train):\n",
    "    \n",
    "#     # create a numpy array to store out of folds predictions\n",
    "#     oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "#     kfold = StratifiedKFold(\n",
    "#         n_splits=n_folds, \n",
    "#         shuffle=True, \n",
    "#         random_state=seed\n",
    "#     )\n",
    "    \n",
    "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "#         print(\"#\" * 50)\n",
    "#         print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "#         x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "#         y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "        \n",
    "#         lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "#         lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "#         model = lgb.train(\n",
    "#             params=params,\n",
    "#             train_set=lgb_train,\n",
    "#             num_boost_round=10500,\n",
    "#             valid_sets = [lgb_train, lgb_valid],\n",
    "#             early_stopping_rounds=100,\n",
    "#             verbose_eval = 500,\n",
    "#             feval = lgb_amex_metric\n",
    "#         )\n",
    "#         # save best model\n",
    "#         dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "#         # predict validation\n",
    "#         val_pred = model.predict(x_val)\n",
    "        \n",
    "#         # add to out of folds array\n",
    "#         oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "#         # compute fold metric\n",
    "#         score = amex_metric(y_val, val_pred)\n",
    "#         print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "#         del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "#         gc.collect()\n",
    "        \n",
    "#     # compute oof\n",
    "#     score = amex_metric(train[target], oof_predictions)\n",
    "#     print(\"oof score is {}\".format(score))\n",
    "    \n",
    "#     # create a dataframe to store out of folds predictions\n",
    "#     oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "#     oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "#     return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
