{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2235a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecc851",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33084e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2977",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059e9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac950de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56355775",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadfd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = [col for col in train.columns if col not in [\"customer_ID\", \"target\"]]\n",
    "target = \"target\"\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d64bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train):\n",
    "    \n",
    "    # round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == \"float32\") | (train.dtypes == \"float64\")].index)\n",
    "    num_last_cols = [col for col in num_cols if \"last\" in col]\n",
    "    for col in num_last_cols:\n",
    "        train[col + \"_round2\"] = train[col].round(2)\n",
    "    \n",
    "    # create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=n_folds, \n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "        print(\"#\" * 50)\n",
    "        print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb_train,\n",
    "            num_boost_round=10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "        )\n",
    "        # save best model\n",
    "        dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "        # predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        \n",
    "        # add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "        # compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        \n",
    "    # compute oof\n",
    "    score = amex_metric(train[target], oof_predictions)\n",
    "    print(\"oof score is {}\".format(score))\n",
    "    \n",
    "    # create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "    oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0f7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 0 with 918 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.772526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 148522\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py:183: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's binary_logloss: 0.339401\ttraining's amex_metric: 0.776587\tvalid_1's binary_logloss: 0.342842\tvalid_1's amex_metric: 0.760569\n",
      "[1000]\ttraining's binary_logloss: 0.24805\ttraining's amex_metric: 0.793409\tvalid_1's binary_logloss: 0.255314\tvalid_1's amex_metric: 0.771594\n",
      "[1500]\ttraining's binary_logloss: 0.223958\ttraining's amex_metric: 0.806084\tvalid_1's binary_logloss: 0.234848\tvalid_1's amex_metric: 0.778971\n",
      "[2000]\ttraining's binary_logloss: 0.210676\ttraining's amex_metric: 0.818282\tvalid_1's binary_logloss: 0.226035\tvalid_1's amex_metric: 0.783681\n",
      "[2500]\ttraining's binary_logloss: 0.204051\ttraining's amex_metric: 0.827902\tvalid_1's binary_logloss: 0.223059\tvalid_1's amex_metric: 0.78678\n",
      "[3000]\ttraining's binary_logloss: 0.197475\ttraining's amex_metric: 0.837146\tvalid_1's binary_logloss: 0.2208\tvalid_1's amex_metric: 0.787978\n",
      "[3500]\ttraining's binary_logloss: 0.19142\ttraining's amex_metric: 0.846478\tvalid_1's binary_logloss: 0.219273\tvalid_1's amex_metric: 0.788952\n",
      "[4000]\ttraining's binary_logloss: 0.186145\ttraining's amex_metric: 0.854718\tvalid_1's binary_logloss: 0.21843\tvalid_1's amex_metric: 0.790095\n",
      "[4500]\ttraining's binary_logloss: 0.180967\ttraining's amex_metric: 0.863197\tvalid_1's binary_logloss: 0.217684\tvalid_1's amex_metric: 0.79025\n",
      "[5000]\ttraining's binary_logloss: 0.175858\ttraining's amex_metric: 0.872315\tvalid_1's binary_logloss: 0.217102\tvalid_1's amex_metric: 0.791318\n",
      "[5500]\ttraining's binary_logloss: 0.171331\ttraining's amex_metric: 0.880026\tvalid_1's binary_logloss: 0.21675\tvalid_1's amex_metric: 0.792082\n",
      "[6000]\ttraining's binary_logloss: 0.167409\ttraining's amex_metric: 0.8875\tvalid_1's binary_logloss: 0.216553\tvalid_1's amex_metric: 0.792925\n",
      "[6500]\ttraining's binary_logloss: 0.163297\ttraining's amex_metric: 0.894295\tvalid_1's binary_logloss: 0.216322\tvalid_1's amex_metric: 0.793218\n",
      "[7000]\ttraining's binary_logloss: 0.158579\ttraining's amex_metric: 0.90216\tvalid_1's binary_logloss: 0.216011\tvalid_1's amex_metric: 0.793448\n",
      "[7500]\ttraining's binary_logloss: 0.154035\ttraining's amex_metric: 0.910017\tvalid_1's binary_logloss: 0.215853\tvalid_1's amex_metric: 0.793594\n",
      "[8000]\ttraining's binary_logloss: 0.150009\ttraining's amex_metric: 0.916414\tvalid_1's binary_logloss: 0.21572\tvalid_1's amex_metric: 0.793946\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0efb35bc52cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-dbc64dfc5228>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(train)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mlgb_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mlgb_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         model = lgb.train(\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2641\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2643\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2644\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2645\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b5d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
