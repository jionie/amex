{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_base_shifted.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = [col for col in train.columns if col not in [\"customer_ID\", \"target\"]]\n",
    "features = [col for col in features if (\"D_87\" not in col)]\n",
    "\n",
    "target = \"target\"\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train):\n",
    "    \n",
    "    # create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=n_folds, \n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "        print(\"#\" * 50)\n",
    "        print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "        x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "        y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb_train,\n",
    "            num_boost_round=10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "        )\n",
    "        # save best model\n",
    "        dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "        # predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        \n",
    "        # add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "        # compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        \n",
    "    # compute oof\n",
    "    score = amex_metric(train[target], oof_predictions)\n",
    "    print(\"oof score is {}\".format(score))\n",
    "    \n",
    "    # create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "    oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "    return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 0 with 1578 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/data2/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/data2/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.722905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 254142\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/data2/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    }
   ],
   "source": [
    "training(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
