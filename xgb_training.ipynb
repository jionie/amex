{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_full_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(\"selected_features.pkl\")\n",
    "\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "cat_features = [feature for feature in cat_features if feature in features]\n",
    "            \n",
    "target = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>B_30_count</th>\n",
       "      <th>B_30_last</th>\n",
       "      <th>B_30_nunique</th>\n",
       "      <th>B_38_count</th>\n",
       "      <th>B_38_last</th>\n",
       "      <th>B_38_nunique</th>\n",
       "      <th>D_114_count</th>\n",
       "      <th>D_114_last</th>\n",
       "      <th>D_114_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>D_137_drawup_duration</th>\n",
       "      <th>D_138_drawup_duration</th>\n",
       "      <th>D_139_drawup_duration</th>\n",
       "      <th>D_140_drawup_duration</th>\n",
       "      <th>D_141_drawup_duration</th>\n",
       "      <th>D_142_drawup_duration</th>\n",
       "      <th>D_143_drawup_duration</th>\n",
       "      <th>D_144_drawup_duration</th>\n",
       "      <th>D_145_drawup_duration</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9223358381327749917</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9223193039457028513</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223189665817919541</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9223188534444851899</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9223173911659837606</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           customer_ID  B_30_count  B_30_last  B_30_nunique  B_38_count  \\\n",
       "0 -9223358381327749917          13          0             2          13   \n",
       "1 -9223193039457028513          13          0             1          13   \n",
       "2 -9223189665817919541          13          0             1          13   \n",
       "3 -9223188534444851899          13          0             1          13   \n",
       "4 -9223173911659837606          13          1             1          13   \n",
       "\n",
       "   B_38_last  B_38_nunique  D_114_count  D_114_last  D_114_nunique  ...  \\\n",
       "0          6             3           13           1              1  ...   \n",
       "1          0             1           13           1              1  ...   \n",
       "2          0             1           13           0              1  ...   \n",
       "3          0             1           13           0              1  ...   \n",
       "4          6             2           13           1              2  ...   \n",
       "\n",
       "   D_137_drawup_duration  D_138_drawup_duration  D_139_drawup_duration  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.0                    0.0   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    4.0                    4.0                    0.0   \n",
       "\n",
       "   D_140_drawup_duration  D_141_drawup_duration  D_142_drawup_duration  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    7.0                   12.0   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   D_143_drawup_duration  D_144_drawup_duration  D_145_drawup_duration  target  \n",
       "0                    0.0                    3.0                    0.0       1  \n",
       "1                    0.0                    1.0                    0.0       0  \n",
       "2                    0.0                    9.0                    0.0       0  \n",
       "3                    0.0                    3.0                    0.0       0  \n",
       "4                    0.0                    4.0                    0.0       1  \n",
       "\n",
       "[5 rows x 3398 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[\"customer_ID\"] + features + [\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [04:41<00:00,  2.50it/s]\n",
      "100%|██████████| 232/232 [00:31<00:00,  7.46it/s]\n"
     ]
    }
   ],
   "source": [
    "float_cols = train[features].select_dtypes(include=[np.float64]).columns\n",
    "for float_col in tqdm(float_cols):\n",
    "    train[float_col] = train[float_col].astype(np.float32)\n",
    "\n",
    "int_cols = train[features].select_dtypes(include=[np.int64]).columns\n",
    "for int_col in tqdm(int_cols):\n",
    "    train[int_col] = train[int_col].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "int16\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "int16\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "int16\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int16\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "int8\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "for col in features:\n",
    "    print(train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"../input/train_select_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "xgb_parms = { \n",
    "    \"max_depth\":4, \n",
    "    \"learning_rate\":0.05, \n",
    "    \"subsample\":0.8,\n",
    "    \"colsample_bytree\":0.6, \n",
    "    \"eval_metric\":\"logloss\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"tree_method\":\"gpu_hist\",\n",
    "    \"gpu_id\":1,\n",
    "    \"predictor\":\"gpu_predictor\",\n",
    "    \"random_state\":seed\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDED WITH DeviceQuantileDMatrix BELOW\n",
    "class IterLoadForDMatrix(xgb.core.DataIter):\n",
    "    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.df = df\n",
    "        self.it = 0 # set iterator to 0\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n",
    "        super().__init__()\n",
    "\n",
    "    def reset(self):\n",
    "        '''Reset the iterator'''\n",
    "        self.it = 0\n",
    "\n",
    "    def next(self, input_data):\n",
    "        '''Yield next batch of data.'''\n",
    "        if self.it == self.batches:\n",
    "            return 0 # Return 0 when there's no more batch.\n",
    "        \n",
    "        a = self.it * self.batch_size\n",
    "        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n",
    "        dt = pd.DataFrame(self.df.iloc[a:b])\n",
    "        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n",
    "        self.it += 1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train):\n",
    "\n",
    "    importances = []\n",
    "    \n",
    "    # create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=n_folds, \n",
    "        shuffle=True, \n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    for fold,(trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "\n",
    "        print(\"#\"*100)\n",
    "        print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "        x_train= train.loc[trn_ind, features]\n",
    "        y_train= train.loc[trn_ind, target]\n",
    "        x_val = train.loc[val_ind, features]\n",
    "        y_val = train.loc[val_ind, target]\n",
    "\n",
    "        # xgb_train = xgb.DeviceQuantileDMatrix(xy_train, max_bin=256)\n",
    "        xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n",
    "        xgb_val = xgb.DMatrix(data=x_val, label=y_val)\n",
    "\n",
    "        model = xgb.train(\n",
    "            xgb_parms, \n",
    "            dtrain=xgb_train,\n",
    "            evals=[(xgb_train,\"train\"),(xgb_val,\"valid\")],\n",
    "            num_boost_round=9999,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        ) \n",
    "        model.save_model(\"../ckpt/xgb_{}_{}.xgb\".format(fold, seed))\n",
    "\n",
    "        # importance\n",
    "        importance = model.get_score(importance_type=\"weight\")\n",
    "        importances.append(pd.DataFrame({\"feature\":importance.keys(), \"importance_{}\".format(fold):importance.values()}))\n",
    "\n",
    "        # oof\n",
    "        oof_preds = model.predict(xgb_val)\n",
    "        score = amex_metric_mod(y_val.values, oof_preds)\n",
    "        print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "        # add to out of folds array\n",
    "        oof_predictions[val_ind] = oof_preds\n",
    "\n",
    "        del x_train, y_train, x_val, y_val, xgb_train, xgb_val, model, importance, oof_preds\n",
    "        _ = gc.collect()\n",
    "    \n",
    "    # compute oof\n",
    "    score = amex_metric_mod(train[target], oof_predictions)\n",
    "    print(\"oof score is {}\".format(score))\n",
    "    \n",
    "    # create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "    oof_df.to_parquet(\"xgb_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "    return importances, oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Training fold 0 with 1950 features...\n",
      "[0]\ttrain-logloss:0.66257\tvalid-logloss:0.66262\n",
      "[100]\ttrain-logloss:0.23578\tvalid-logloss:0.23881\n",
      "[200]\ttrain-logloss:0.22149\tvalid-logloss:0.22699\n",
      "[300]\ttrain-logloss:0.21541\tvalid-logloss:0.22329\n",
      "[400]\ttrain-logloss:0.21124\tvalid-logloss:0.22138\n",
      "[500]\ttrain-logloss:0.20785\tvalid-logloss:0.22026\n",
      "[600]\ttrain-logloss:0.20493\tvalid-logloss:0.21954\n",
      "[700]\ttrain-logloss:0.20235\tvalid-logloss:0.21900\n",
      "[800]\ttrain-logloss:0.19985\tvalid-logloss:0.21853\n",
      "[900]\ttrain-logloss:0.19755\tvalid-logloss:0.21811\n",
      "[1000]\ttrain-logloss:0.19538\tvalid-logloss:0.21787\n",
      "[1100]\ttrain-logloss:0.19326\tvalid-logloss:0.21772\n",
      "[1200]\ttrain-logloss:0.19121\tvalid-logloss:0.21756\n",
      "[1300]\ttrain-logloss:0.18923\tvalid-logloss:0.21747\n",
      "[1400]\ttrain-logloss:0.18720\tvalid-logloss:0.21740\n",
      "[1500]\ttrain-logloss:0.18532\tvalid-logloss:0.21736\n",
      "[1600]\ttrain-logloss:0.18348\tvalid-logloss:0.21729\n",
      "[1700]\ttrain-logloss:0.18166\tvalid-logloss:0.21724\n",
      "[1800]\ttrain-logloss:0.17992\tvalid-logloss:0.21722\n",
      "[1900]\ttrain-logloss:0.17816\tvalid-logloss:0.21720\n",
      "[2000]\ttrain-logloss:0.17639\tvalid-logloss:0.21721\n",
      "[2100]\ttrain-logloss:0.17476\tvalid-logloss:0.21722\n",
      "[2153]\ttrain-logloss:0.17390\tvalid-logloss:0.21721\n",
      "fold 0 score is 0.791644034462486\n",
      "####################################################################################################\n",
      "Training fold 1 with 1950 features...\n",
      "[0]\ttrain-logloss:0.66255\tvalid-logloss:0.66255\n",
      "[100]\ttrain-logloss:0.23573\tvalid-logloss:0.23890\n",
      "[200]\ttrain-logloss:0.22139\tvalid-logloss:0.22697\n",
      "[300]\ttrain-logloss:0.21535\tvalid-logloss:0.22325\n",
      "[400]\ttrain-logloss:0.21114\tvalid-logloss:0.22133\n",
      "[500]\ttrain-logloss:0.20777\tvalid-logloss:0.22027\n",
      "[600]\ttrain-logloss:0.20488\tvalid-logloss:0.21954\n",
      "[700]\ttrain-logloss:0.20227\tvalid-logloss:0.21898\n",
      "[800]\ttrain-logloss:0.19981\tvalid-logloss:0.21852\n",
      "[900]\ttrain-logloss:0.19753\tvalid-logloss:0.21822\n",
      "[1000]\ttrain-logloss:0.19540\tvalid-logloss:0.21798\n",
      "[1100]\ttrain-logloss:0.19331\tvalid-logloss:0.21777\n",
      "[1200]\ttrain-logloss:0.19124\tvalid-logloss:0.21759\n",
      "[1300]\ttrain-logloss:0.18925\tvalid-logloss:0.21749\n",
      "[1400]\ttrain-logloss:0.18736\tvalid-logloss:0.21746\n",
      "[1500]\ttrain-logloss:0.18545\tvalid-logloss:0.21733\n",
      "[1600]\ttrain-logloss:0.18359\tvalid-logloss:0.21728\n",
      "[1700]\ttrain-logloss:0.18175\tvalid-logloss:0.21722\n",
      "[1800]\ttrain-logloss:0.18003\tvalid-logloss:0.21718\n",
      "[1900]\ttrain-logloss:0.17829\tvalid-logloss:0.21711\n",
      "[2000]\ttrain-logloss:0.17659\tvalid-logloss:0.21707\n",
      "[2100]\ttrain-logloss:0.17490\tvalid-logloss:0.21710\n",
      "[2101]\ttrain-logloss:0.17488\tvalid-logloss:0.21709\n",
      "fold 1 score is 0.7929260136551135\n",
      "####################################################################################################\n",
      "Training fold 2 with 1950 features...\n",
      "[0]\ttrain-logloss:0.66253\tvalid-logloss:0.66266\n",
      "[100]\ttrain-logloss:0.23603\tvalid-logloss:0.23871\n",
      "[200]\ttrain-logloss:0.22169\tvalid-logloss:0.22653\n",
      "[300]\ttrain-logloss:0.21553\tvalid-logloss:0.22251\n",
      "[400]\ttrain-logloss:0.21134\tvalid-logloss:0.22050\n",
      "[500]\ttrain-logloss:0.20792\tvalid-logloss:0.21929\n",
      "[600]\ttrain-logloss:0.20492\tvalid-logloss:0.21850\n",
      "[700]\ttrain-logloss:0.20226\tvalid-logloss:0.21799\n",
      "[800]\ttrain-logloss:0.19981\tvalid-logloss:0.21761\n",
      "[900]\ttrain-logloss:0.19753\tvalid-logloss:0.21740\n",
      "[1000]\ttrain-logloss:0.19531\tvalid-logloss:0.21714\n",
      "[1100]\ttrain-logloss:0.19316\tvalid-logloss:0.21694\n",
      "[1200]\ttrain-logloss:0.19119\tvalid-logloss:0.21677\n",
      "[1300]\ttrain-logloss:0.18922\tvalid-logloss:0.21668\n",
      "[1400]\ttrain-logloss:0.18730\tvalid-logloss:0.21656\n",
      "[1500]\ttrain-logloss:0.18542\tvalid-logloss:0.21654\n",
      "[1600]\ttrain-logloss:0.18361\tvalid-logloss:0.21644\n",
      "[1700]\ttrain-logloss:0.18182\tvalid-logloss:0.21642\n",
      "[1800]\ttrain-logloss:0.18010\tvalid-logloss:0.21636\n",
      "[1900]\ttrain-logloss:0.17836\tvalid-logloss:0.21632\n",
      "[2000]\ttrain-logloss:0.17665\tvalid-logloss:0.21631\n",
      "[2017]\ttrain-logloss:0.17637\tvalid-logloss:0.21631\n",
      "fold 2 score is 0.7952708270594084\n",
      "####################################################################################################\n",
      "Training fold 3 with 1950 features...\n",
      "[0]\ttrain-logloss:0.66258\tvalid-logloss:0.66270\n",
      "[100]\ttrain-logloss:0.23568\tvalid-logloss:0.23956\n",
      "[200]\ttrain-logloss:0.22130\tvalid-logloss:0.22778\n",
      "[300]\ttrain-logloss:0.21522\tvalid-logloss:0.22403\n",
      "[400]\ttrain-logloss:0.21107\tvalid-logloss:0.22209\n",
      "[500]\ttrain-logloss:0.20768\tvalid-logloss:0.22089\n",
      "[600]\ttrain-logloss:0.20473\tvalid-logloss:0.22009\n",
      "[700]\ttrain-logloss:0.20210\tvalid-logloss:0.21944\n",
      "[800]\ttrain-logloss:0.19968\tvalid-logloss:0.21911\n",
      "[900]\ttrain-logloss:0.19739\tvalid-logloss:0.21875\n",
      "[1000]\ttrain-logloss:0.19521\tvalid-logloss:0.21850\n",
      "[1100]\ttrain-logloss:0.19317\tvalid-logloss:0.21830\n",
      "[1200]\ttrain-logloss:0.19122\tvalid-logloss:0.21812\n",
      "[1300]\ttrain-logloss:0.18928\tvalid-logloss:0.21793\n",
      "[1400]\ttrain-logloss:0.18732\tvalid-logloss:0.21783\n",
      "[1495]\ttrain-logloss:0.18556\tvalid-logloss:0.21781\n",
      "fold 3 score is 0.793509580600231\n",
      "####################################################################################################\n",
      "Training fold 4 with 1950 features...\n",
      "[0]\ttrain-logloss:0.66263\tvalid-logloss:0.66249\n",
      "[100]\ttrain-logloss:0.23637\tvalid-logloss:0.23631\n",
      "[200]\ttrain-logloss:0.22197\tvalid-logloss:0.22455\n",
      "[300]\ttrain-logloss:0.21580\tvalid-logloss:0.22102\n",
      "[400]\ttrain-logloss:0.21170\tvalid-logloss:0.21928\n",
      "[500]\ttrain-logloss:0.20833\tvalid-logloss:0.21813\n",
      "[600]\ttrain-logloss:0.20545\tvalid-logloss:0.21745\n",
      "[700]\ttrain-logloss:0.20282\tvalid-logloss:0.21696\n",
      "[800]\ttrain-logloss:0.20036\tvalid-logloss:0.21646\n",
      "[900]\ttrain-logloss:0.19803\tvalid-logloss:0.21621\n",
      "[1000]\ttrain-logloss:0.19586\tvalid-logloss:0.21597\n",
      "[1100]\ttrain-logloss:0.19380\tvalid-logloss:0.21577\n",
      "[1200]\ttrain-logloss:0.19166\tvalid-logloss:0.21567\n",
      "[1300]\ttrain-logloss:0.18971\tvalid-logloss:0.21558\n",
      "[1400]\ttrain-logloss:0.18779\tvalid-logloss:0.21548\n",
      "[1500]\ttrain-logloss:0.18597\tvalid-logloss:0.21548\n",
      "[1518]\ttrain-logloss:0.18561\tvalid-logloss:0.21546\n",
      "fold 4 score is 0.7942605198615638\n",
      "oof score is 0.7936078671861629\n"
     ]
    }
   ],
   "source": [
    "importances, oof_df = training(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = importances[0].copy()\n",
    "for k in range(1, n_folds): \n",
    "    importance = importance.merge(importances[k], on=\"feature\", how=\"left\")\n",
    "    \n",
    "importance[\"importance\"] = importance.iloc[:,1:].mean(axis=1)\n",
    "importance = importance.sort_values(\"importance\",ascending=False)\n",
    "\n",
    "# importance.to_csv(\"xgb_feature_importance.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "\n",
    "plt.figure(figsize=(10, n_features))\n",
    "plt.barh(np.arange(n_features, 0, -1), importance.importance.values[:n_features])\n",
    "plt.yticks(np.arange(n_features,0,-1), importance.feature.values[:n_features])\n",
    "plt.title(\"xgb feature importance - Top {}\".format(n_features))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_2 = [col for col in importance[\"feature\"] if \"target\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.loc[importance[\"feature\"].isin(P_2)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
