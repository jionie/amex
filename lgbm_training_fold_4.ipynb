{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_full_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = load(\"selected_features.pkl\")\n",
    "\n",
    "target = \"target\"\n",
    "\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "cat_features = [feature for feature in cat_features if feature in features]\n",
    "            \n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fold = 4\n",
    "\n",
    "kfold = StratifiedKFold(\n",
    "    n_splits=n_folds, \n",
    "    shuffle=True, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "    \n",
    "    if fold == target_fold:\n",
    "        break\n",
    "\n",
    "x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, x_train, x_val, y_train, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(\"../ckpt/lgbm_seed_{}\".format(seed))\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "save_path = os.path.join(save_folder, \"fold_{}\".format(target_fold))\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 4 with 1950 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.201701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211763\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "iteration 0, score= 0.68819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100, score= 0.76033\n",
      "iteration 200, score= 0.76238\n",
      "iteration 300, score= 0.76329\n",
      "iteration 400, score= 0.76529\n",
      "[500]\ttraining's binary_logloss: 0.337611\ttraining's amex_metric: 0.777464\tvalid_1's binary_logloss: 0.339416\tvalid_1's amex_metric: 0.768159\n",
      "iteration 500, score= 0.76811\n",
      "iteration 600, score= 0.77025\n",
      "iteration 700, score= 0.77244\n",
      "iteration 800, score= 0.77405\n",
      "iteration 900, score= 0.77648\n",
      "[1000]\ttraining's binary_logloss: 0.246655\ttraining's amex_metric: 0.794357\tvalid_1's binary_logloss: 0.251986\tvalid_1's amex_metric: 0.778221\n",
      "iteration 1000, score= 0.77822\n",
      "iteration 1100, score= 0.77990\n",
      "iteration 1200, score= 0.78129\n",
      "iteration 1300, score= 0.78295\n",
      "iteration 1400, score= 0.78445\n",
      "[1500]\ttraining's binary_logloss: 0.222494\ttraining's amex_metric: 0.807433\tvalid_1's binary_logloss: 0.231772\tvalid_1's amex_metric: 0.785057\n",
      "iteration 1500, score= 0.78495\n",
      "iteration 1600, score= 0.78727\n",
      "iteration 1700, score= 0.78813\n",
      "iteration 1800, score= 0.78876\n",
      "iteration 1900, score= 0.78886\n",
      "[2000]\ttraining's binary_logloss: 0.208892\ttraining's amex_metric: 0.820563\tvalid_1's binary_logloss: 0.223045\tvalid_1's amex_metric: 0.790165\n",
      "iteration 2000, score= 0.79021\n",
      "iteration 2100, score= 0.79064\n",
      "iteration 2200, score= 0.79156\n",
      "iteration 2300, score= 0.79166\n",
      "iteration 2400, score= 0.79238\n",
      "High Score: iteration 2439, score=0.79308\n",
      "High Score: iteration 2442, score=0.79315\n",
      "High Score: iteration 2443, score=0.79315\n",
      "High Score: iteration 2444, score=0.79315\n",
      "High Score: iteration 2445, score=0.79315\n",
      "High Score: iteration 2447, score=0.79330\n",
      "High Score: iteration 2448, score=0.79330\n",
      "High Score: iteration 2475, score=0.79341\n",
      "High Score: iteration 2476, score=0.79343\n",
      "High Score: iteration 2477, score=0.79347\n",
      "High Score: iteration 2478, score=0.79352\n",
      "High Score: iteration 2479, score=0.79354\n",
      "[2500]\ttraining's binary_logloss: 0.201949\ttraining's amex_metric: 0.830603\tvalid_1's binary_logloss: 0.220195\tvalid_1's amex_metric: 0.792944\n",
      "iteration 2500, score= 0.79294\n",
      "iteration 2600, score= 0.79331\n",
      "High Score: iteration 2602, score=0.79363\n",
      "High Score: iteration 2607, score=0.79364\n",
      "High Score: iteration 2608, score=0.79364\n",
      "High Score: iteration 2628, score=0.79372\n",
      "High Score: iteration 2629, score=0.79372\n",
      "High Score: iteration 2630, score=0.79376\n",
      "High Score: iteration 2631, score=0.79378\n",
      "High Score: iteration 2632, score=0.79378\n",
      "High Score: iteration 2633, score=0.79378\n",
      "High Score: iteration 2634, score=0.79389\n",
      "High Score: iteration 2635, score=0.79391\n",
      "High Score: iteration 2637, score=0.79391\n",
      "High Score: iteration 2638, score=0.79398\n",
      "High Score: iteration 2646, score=0.79406\n",
      "High Score: iteration 2657, score=0.79419\n",
      "High Score: iteration 2674, score=0.79420\n",
      "iteration 2700, score= 0.79400\n",
      "High Score: iteration 2714, score=0.79422\n",
      "High Score: iteration 2715, score=0.79432\n",
      "High Score: iteration 2716, score=0.79432\n",
      "High Score: iteration 2721, score=0.79434\n",
      "High Score: iteration 2760, score=0.79456\n",
      "High Score: iteration 2762, score=0.79456\n",
      "High Score: iteration 2775, score=0.79458\n",
      "High Score: iteration 2776, score=0.79468\n",
      "High Score: iteration 2779, score=0.79470\n",
      "High Score: iteration 2780, score=0.79470\n",
      "High Score: iteration 2781, score=0.79473\n",
      "High Score: iteration 2782, score=0.79477\n",
      "iteration 2800, score= 0.79460\n",
      "High Score: iteration 2834, score=0.79479\n",
      "High Score: iteration 2835, score=0.79483\n",
      "High Score: iteration 2841, score=0.79484\n",
      "iteration 2900, score= 0.79470\n",
      "High Score: iteration 2917, score=0.79486\n",
      "High Score: iteration 2943, score=0.79486\n",
      "High Score: iteration 2944, score=0.79486\n",
      "High Score: iteration 2945, score=0.79486\n",
      "High Score: iteration 2948, score=0.79486\n",
      "High Score: iteration 2949, score=0.79486\n",
      "High Score: iteration 2950, score=0.79493\n",
      "High Score: iteration 2968, score=0.79494\n",
      "High Score: iteration 2977, score=0.79497\n",
      "High Score: iteration 2993, score=0.79511\n",
      "[3000]\ttraining's binary_logloss: 0.195069\ttraining's amex_metric: 0.839999\tvalid_1's binary_logloss: 0.217961\tvalid_1's amex_metric: 0.795032\n",
      "iteration 3000, score= 0.79507\n",
      "High Score: iteration 3004, score=0.79522\n",
      "High Score: iteration 3005, score=0.79525\n",
      "High Score: iteration 3006, score=0.79525\n",
      "High Score: iteration 3010, score=0.79525\n",
      "High Score: iteration 3011, score=0.79525\n",
      "High Score: iteration 3012, score=0.79525\n",
      "High Score: iteration 3016, score=0.79536\n",
      "High Score: iteration 3017, score=0.79536\n",
      "High Score: iteration 3018, score=0.79538\n",
      "High Score: iteration 3024, score=0.79555\n",
      "iteration 3100, score= 0.79512\n",
      "High Score: iteration 3175, score=0.79567\n",
      "iteration 3200, score= 0.79510\n",
      "High Score: iteration 3295, score=0.79568\n",
      "iteration 3300, score= 0.79554\n",
      "High Score: iteration 3307, score=0.79569\n",
      "High Score: iteration 3308, score=0.79569\n",
      "High Score: iteration 3309, score=0.79569\n",
      "High Score: iteration 3322, score=0.79572\n",
      "High Score: iteration 3323, score=0.79572\n",
      "High Score: iteration 3324, score=0.79572\n",
      "High Score: iteration 3326, score=0.79573\n",
      "High Score: iteration 3328, score=0.79575\n",
      "High Score: iteration 3329, score=0.79577\n",
      "High Score: iteration 3330, score=0.79577\n",
      "High Score: iteration 3331, score=0.79584\n",
      "High Score: iteration 3335, score=0.79592\n",
      "High Score: iteration 3340, score=0.79597\n",
      "High Score: iteration 3343, score=0.79599\n",
      "High Score: iteration 3344, score=0.79599\n",
      "High Score: iteration 3357, score=0.79608\n",
      "High Score: iteration 3386, score=0.79610\n",
      "High Score: iteration 3387, score=0.79614\n",
      "High Score: iteration 3388, score=0.79618\n",
      "High Score: iteration 3397, score=0.79619\n",
      "iteration 3400, score= 0.79604\n",
      "High Score: iteration 3421, score=0.79624\n",
      "High Score: iteration 3426, score=0.79654\n",
      "High Score: iteration 3428, score=0.79658\n",
      "High Score: iteration 3429, score=0.79663\n",
      "High Score: iteration 3430, score=0.79665\n",
      "High Score: iteration 3444, score=0.79666\n",
      "High Score: iteration 3463, score=0.79670\n",
      "High Score: iteration 3464, score=0.79672\n",
      "High Score: iteration 3465, score=0.79672\n",
      "High Score: iteration 3466, score=0.79672\n",
      "High Score: iteration 3467, score=0.79672\n",
      "High Score: iteration 3468, score=0.79672\n",
      "High Score: iteration 3469, score=0.79672\n",
      "High Score: iteration 3470, score=0.79672\n",
      "[3500]\ttraining's binary_logloss: 0.188743\ttraining's amex_metric: 0.85011\tvalid_1's binary_logloss: 0.216486\tvalid_1's amex_metric: 0.79663\n",
      "iteration 3500, score= 0.79659\n",
      "High Score: iteration 3503, score=0.79689\n",
      "High Score: iteration 3511, score=0.79689\n",
      "High Score: iteration 3577, score=0.79698\n",
      "High Score: iteration 3578, score=0.79698\n",
      "High Score: iteration 3579, score=0.79698\n",
      "iteration 3600, score= 0.79642\n",
      "iteration 3700, score= 0.79644\n",
      "High Score: iteration 3777, score=0.79700\n",
      "High Score: iteration 3778, score=0.79706\n",
      "iteration 3800, score= 0.79678\n",
      "iteration 3900, score= 0.79682\n",
      "[4000]\ttraining's binary_logloss: 0.18317\ttraining's amex_metric: 0.85966\tvalid_1's binary_logloss: 0.215529\tvalid_1's amex_metric: 0.796565\n",
      "iteration 4000, score= 0.79667\n",
      "High Score: iteration 4021, score=0.79706\n",
      "High Score: iteration 4022, score=0.79712\n",
      "High Score: iteration 4023, score=0.79714\n",
      "High Score: iteration 4058, score=0.79716\n",
      "High Score: iteration 4064, score=0.79729\n",
      "High Score: iteration 4065, score=0.79729\n",
      "High Score: iteration 4066, score=0.79729\n",
      "iteration 4100, score= 0.79711\n",
      "High Score: iteration 4102, score=0.79730\n",
      "High Score: iteration 4107, score=0.79732\n",
      "High Score: iteration 4108, score=0.79732\n",
      "High Score: iteration 4109, score=0.79732\n",
      "High Score: iteration 4110, score=0.79732\n",
      "High Score: iteration 4111, score=0.79747\n",
      "High Score: iteration 4112, score=0.79747\n",
      "High Score: iteration 4125, score=0.79748\n",
      "High Score: iteration 4132, score=0.79753\n",
      "High Score: iteration 4140, score=0.79759\n",
      "iteration 4200, score= 0.79721\n",
      "iteration 4300, score= 0.79701\n",
      "iteration 4400, score= 0.79727\n",
      "[4500]\ttraining's binary_logloss: 0.177738\ttraining's amex_metric: 0.869106\tvalid_1's binary_logloss: 0.214864\tvalid_1's amex_metric: 0.79746\n",
      "iteration 4500, score= 0.79735\n",
      "High Score: iteration 4505, score=0.79761\n",
      "High Score: iteration 4527, score=0.79762\n",
      "High Score: iteration 4528, score=0.79771\n",
      "High Score: iteration 4530, score=0.79785\n",
      "iteration 4600, score= 0.79748\n",
      "iteration 4700, score= 0.79746\n",
      "iteration 4800, score= 0.79744\n",
      "iteration 4900, score= 0.79750\n",
      "[5000]\ttraining's binary_logloss: 0.172355\ttraining's amex_metric: 0.87779\tvalid_1's binary_logloss: 0.21431\tvalid_1's amex_metric: 0.797674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000, score= 0.79772\n",
      "High Score: iteration 5007, score=0.79786\n",
      "High Score: iteration 5027, score=0.79791\n",
      "High Score: iteration 5028, score=0.79791\n",
      "iteration 5100, score= 0.79759\n",
      "High Score: iteration 5123, score=0.79793\n",
      "High Score: iteration 5138, score=0.79793\n",
      "High Score: iteration 5139, score=0.79793\n",
      "High Score: iteration 5140, score=0.79793\n",
      "High Score: iteration 5143, score=0.79804\n",
      "iteration 5200, score= 0.79779\n",
      "iteration 5300, score= 0.79775\n",
      "iteration 5400, score= 0.79775\n",
      "[5500]\ttraining's binary_logloss: 0.167621\ttraining's amex_metric: 0.885906\tvalid_1's binary_logloss: 0.21391\tvalid_1's amex_metric: 0.797736\n",
      "iteration 5500, score= 0.79772\n",
      "High Score: iteration 5553, score=0.79806\n",
      "High Score: iteration 5566, score=0.79806\n",
      "iteration 5600, score= 0.79802\n",
      "High Score: iteration 5606, score=0.79807\n",
      "High Score: iteration 5610, score=0.79815\n",
      "High Score: iteration 5611, score=0.79815\n",
      "High Score: iteration 5612, score=0.79817\n",
      "High Score: iteration 5655, score=0.79818\n",
      "High Score: iteration 5656, score=0.79818\n",
      "High Score: iteration 5657, score=0.79824\n",
      "High Score: iteration 5690, score=0.79826\n",
      "High Score: iteration 5699, score=0.79829\n",
      "iteration 5700, score= 0.79839\n",
      "High Score: iteration 5700, score=0.79839\n",
      "High Score: iteration 5701, score=0.79839\n",
      "High Score: iteration 5702, score=0.79841\n",
      "High Score: iteration 5712, score=0.79843\n",
      "High Score: iteration 5713, score=0.79845\n",
      "High Score: iteration 5714, score=0.79856\n",
      "High Score: iteration 5718, score=0.79860\n",
      "High Score: iteration 5719, score=0.79860\n",
      "High Score: iteration 5743, score=0.79873\n",
      "High Score: iteration 5745, score=0.79880\n",
      "High Score: iteration 5746, score=0.79882\n",
      "High Score: iteration 5751, score=0.79884\n",
      "High Score: iteration 5753, score=0.79884\n",
      "High Score: iteration 5754, score=0.79884\n",
      "High Score: iteration 5755, score=0.79886\n",
      "iteration 5800, score= 0.79862\n",
      "High Score: iteration 5822, score=0.79887\n",
      "High Score: iteration 5823, score=0.79892\n",
      "High Score: iteration 5824, score=0.79892\n",
      "High Score: iteration 5834, score=0.79898\n",
      "High Score: iteration 5838, score=0.79909\n",
      "High Score: iteration 5839, score=0.79913\n",
      "High Score: iteration 5846, score=0.79915\n",
      "High Score: iteration 5847, score=0.79915\n",
      "High Score: iteration 5880, score=0.79919\n",
      "High Score: iteration 5881, score=0.79930\n",
      "iteration 5900, score= 0.79901\n",
      "[6000]\ttraining's binary_logloss: 0.163555\ttraining's amex_metric: 0.893522\tvalid_1's binary_logloss: 0.2137\tvalid_1's amex_metric: 0.798855\n",
      "iteration 6000, score= 0.79892\n",
      "iteration 6100, score= 0.79893\n",
      "iteration 6200, score= 0.79893\n",
      "iteration 6300, score= 0.79895\n",
      "High Score: iteration 6337, score=0.79931\n",
      "High Score: iteration 6338, score=0.79937\n",
      "High Score: iteration 6342, score=0.79937\n",
      "High Score: iteration 6343, score=0.79937\n",
      "iteration 6400, score= 0.79855\n",
      "[6500]\ttraining's binary_logloss: 0.159281\ttraining's amex_metric: 0.900866\tvalid_1's binary_logloss: 0.21347\tvalid_1's amex_metric: 0.798541\n",
      "iteration 6500, score= 0.79854\n",
      "iteration 6600, score= 0.79876\n",
      "iteration 6700, score= 0.79913\n",
      "High Score: iteration 6736, score=0.79940\n",
      "High Score: iteration 6737, score=0.79942\n",
      "High Score: iteration 6748, score=0.79944\n",
      "High Score: iteration 6754, score=0.79950\n",
      "High Score: iteration 6760, score=0.79954\n",
      "iteration 6800, score= 0.79938\n",
      "High Score: iteration 6802, score=0.79965\n",
      "iteration 6900, score= 0.79946\n",
      "[7000]\ttraining's binary_logloss: 0.154334\ttraining's amex_metric: 0.908843\tvalid_1's binary_logloss: 0.213218\tvalid_1's amex_metric: 0.799333\n",
      "iteration 7000, score= 0.79910\n",
      "iteration 7100, score= 0.79919\n",
      "iteration 7200, score= 0.79904\n",
      "iteration 7300, score= 0.79872\n",
      "iteration 7400, score= 0.79868\n",
      "[7500]\ttraining's binary_logloss: 0.149632\ttraining's amex_metric: 0.916316\tvalid_1's binary_logloss: 0.213029\tvalid_1's amex_metric: 0.798837\n",
      "iteration 7500, score= 0.79890\n",
      "iteration 7600, score= 0.79862\n",
      "iteration 7700, score= 0.79884\n",
      "iteration 7800, score= 0.79892\n",
      "iteration 7900, score= 0.79884\n",
      "[8000]\ttraining's binary_logloss: 0.145473\ttraining's amex_metric: 0.923325\tvalid_1's binary_logloss: 0.212862\tvalid_1's amex_metric: 0.798506\n",
      "iteration 8000, score= 0.79851\n",
      "iteration 8100, score= 0.79847\n",
      "iteration 8200, score= 0.79860\n",
      "iteration 8300, score= 0.79851\n",
      "iteration 8400, score= 0.79874\n",
      "[8500]\ttraining's binary_logloss: 0.141884\ttraining's amex_metric: 0.929623\tvalid_1's binary_logloss: 0.212773\tvalid_1's amex_metric: 0.798706\n",
      "iteration 8500, score= 0.79869\n",
      "iteration 8600, score= 0.79840\n",
      "iteration 8700, score= 0.79861\n",
      "iteration 8800, score= 0.79820\n",
      "iteration 8900, score= 0.79875\n",
      "[9000]\ttraining's binary_logloss: 0.137829\ttraining's amex_metric: 0.935542\tvalid_1's binary_logloss: 0.212671\tvalid_1's amex_metric: 0.798249\n",
      "iteration 9000, score= 0.79825\n",
      "iteration 9100, score= 0.79855\n",
      "iteration 9200, score= 0.79834\n",
      "iteration 9300, score= 0.79848\n",
      "iteration 9400, score= 0.79851\n",
      "[9500]\ttraining's binary_logloss: 0.134193\ttraining's amex_metric: 0.940742\tvalid_1's binary_logloss: 0.212617\tvalid_1's amex_metric: 0.798452\n",
      "iteration 9500, score= 0.79845\n",
      "iteration 9600, score= 0.79822\n",
      "iteration 9700, score= 0.79859\n",
      "iteration 9800, score= 0.79861\n",
      "iteration 9900, score= 0.79777\n",
      "[10000]\ttraining's binary_logloss: 0.130619\ttraining's amex_metric: 0.946087\tvalid_1's binary_logloss: 0.212641\tvalid_1's amex_metric: 0.798044\n",
      "iteration 10000, score= 0.79809\n",
      "iteration 10100, score= 0.79788\n",
      "iteration 10200, score= 0.79804\n",
      "iteration 10300, score= 0.79806\n",
      "iteration 10400, score= 0.79792\n",
      "[10500]\ttraining's binary_logloss: 0.127506\ttraining's amex_metric: 0.950745\tvalid_1's binary_logloss: 0.212614\tvalid_1's amex_metric: 0.79795\n",
      "iteration 10500, score= 0.79785\n",
      "iteration 10600, score= 0.79783\n",
      "iteration 10700, score= 0.79797\n",
      "iteration 10800, score= 0.79830\n",
      "iteration 10900, score= 0.79832\n",
      "[11000]\ttraining's binary_logloss: 0.124299\ttraining's amex_metric: 0.955238\tvalid_1's binary_logloss: 0.212574\tvalid_1's amex_metric: 0.798453\n",
      "iteration 11000, score= 0.79847\n",
      "iteration 11100, score= 0.79888\n",
      "iteration 11200, score= 0.79879\n",
      "iteration 11300, score= 0.79838\n",
      "iteration 11400, score= 0.79904\n",
      "[11500]\ttraining's binary_logloss: 0.120984\ttraining's amex_metric: 0.959488\tvalid_1's binary_logloss: 0.21259\tvalid_1's amex_metric: 0.798958\n",
      "iteration 11500, score= 0.79898\n",
      "iteration 11600, score= 0.79904\n",
      "iteration 11700, score= 0.79942\n",
      "High Score: iteration 11721, score=0.79974\n",
      "iteration 11800, score= 0.79926\n",
      "iteration 11900, score= 0.79926\n",
      "[12000]\ttraining's binary_logloss: 0.117973\ttraining's amex_metric: 0.96352\tvalid_1's binary_logloss: 0.212621\tvalid_1's amex_metric: 0.798744\n",
      "iteration 12000, score= 0.79874\n",
      "iteration 12100, score= 0.79883\n",
      "iteration 12200, score= 0.79903\n",
      "iteration 12300, score= 0.79878\n",
      "iteration 12400, score= 0.79885\n",
      "[12500]\ttraining's binary_logloss: 0.115369\ttraining's amex_metric: 0.967092\tvalid_1's binary_logloss: 0.212612\tvalid_1's amex_metric: 0.798791\n",
      "iteration 12500, score= 0.79894\n",
      "iteration 12600, score= 0.79891\n",
      "iteration 12700, score= 0.79907\n",
      "iteration 12800, score= 0.79911\n",
      "iteration 12900, score= 0.79951\n",
      "[13000]\ttraining's binary_logloss: 0.111999\ttraining's amex_metric: 0.970948\tvalid_1's binary_logloss: 0.212658\tvalid_1's amex_metric: 0.799554\n",
      "iteration 13000, score= 0.79958\n",
      "iteration 13100, score= 0.79935\n",
      "iteration 13200, score= 0.79926\n",
      "iteration 13300, score= 0.79935\n",
      "iteration 13400, score= 0.79935\n",
      "High Score: iteration 13495, score=0.79975\n",
      "[13500]\ttraining's binary_logloss: 0.109337\ttraining's amex_metric: 0.974369\tvalid_1's binary_logloss: 0.212697\tvalid_1's amex_metric: 0.799639\n",
      "iteration 13500, score= 0.79964\n",
      "iteration 13600, score= 0.79939\n",
      "iteration 13700, score= 0.79962\n",
      "High Score: iteration 13709, score=0.79977\n",
      "High Score: iteration 13723, score=0.79983\n",
      "High Score: iteration 13744, score=0.79989\n",
      "iteration 13800, score= 0.79962\n",
      "High Score: iteration 13868, score=0.79990\n",
      "High Score: iteration 13869, score=0.79994\n",
      "High Score: iteration 13885, score=0.79996\n",
      "High Score: iteration 13890, score=0.79998\n",
      "High Score: iteration 13898, score=0.79998\n",
      "iteration 13900, score= 0.79988\n",
      "High Score: iteration 13913, score=0.80005\n",
      "High Score: iteration 13914, score=0.80005\n",
      "High Score: iteration 13915, score=0.80007\n",
      "High Score: iteration 13929, score=0.80007\n",
      "[14000]\ttraining's binary_logloss: 0.106364\ttraining's amex_metric: 0.977451\tvalid_1's binary_logloss: 0.212739\tvalid_1's amex_metric: 0.799667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14000, score= 0.79967\n",
      "iteration 14100, score= 0.79941\n",
      "iteration 14200, score= 0.79949\n",
      "iteration 14300, score= 0.79920\n",
      "iteration 14400, score= 0.79922\n",
      "[14500]\ttraining's binary_logloss: 0.104051\ttraining's amex_metric: 0.979689\tvalid_1's binary_logloss: 0.212783\tvalid_1's amex_metric: 0.799582\n",
      "iteration 14500, score= 0.79964\n",
      "iteration 14600, score= 0.79922\n",
      "iteration 14700, score= 0.79942\n",
      "iteration 14800, score= 0.79981\n",
      "iteration 14900, score= 0.79965\n",
      "[15000]\ttraining's binary_logloss: 0.101692\ttraining's amex_metric: 0.98206\tvalid_1's binary_logloss: 0.21283\tvalid_1's amex_metric: 0.799248\n",
      "iteration 15000, score= 0.79927\n",
      "iteration 15100, score= 0.79923\n",
      "iteration 15200, score= 0.79917\n",
      "iteration 15300, score= 0.79904\n",
      "iteration 15400, score= 0.79889\n",
      "[15500]\ttraining's binary_logloss: 0.099218\ttraining's amex_metric: 0.984346\tvalid_1's binary_logloss: 0.212929\tvalid_1's amex_metric: 0.799093\n",
      "iteration 15500, score= 0.79905\n",
      "iteration 15600, score= 0.79922\n",
      "iteration 15700, score= 0.79922\n",
      "iteration 15800, score= 0.79921\n",
      "iteration 15900, score= 0.79926\n",
      "[16000]\ttraining's binary_logloss: 0.096586\ttraining's amex_metric: 0.986427\tvalid_1's binary_logloss: 0.213067\tvalid_1's amex_metric: 0.798897\n",
      "iteration 16000, score= 0.79892\n",
      "iteration 16100, score= 0.79918\n",
      "iteration 16200, score= 0.79901\n",
      "iteration 16300, score= 0.79895\n",
      "iteration 16400, score= 0.79941\n",
      "[16500]\ttraining's binary_logloss: 0.0939935\ttraining's amex_metric: 0.988464\tvalid_1's binary_logloss: 0.213219\tvalid_1's amex_metric: 0.799509\n",
      "iteration 16500, score= 0.79953\n",
      "iteration 16600, score= 0.79952\n",
      "iteration 16700, score= 0.79902\n",
      "iteration 16800, score= 0.79912\n",
      "iteration 16900, score= 0.79890\n",
      "[17000]\ttraining's binary_logloss: 0.0918574\ttraining's amex_metric: 0.990014\tvalid_1's binary_logloss: 0.213279\tvalid_1's amex_metric: 0.79894\n",
      "iteration 17000, score= 0.79898\n",
      "iteration 17100, score= 0.79911\n",
      "iteration 17200, score= 0.79894\n",
      "iteration 17300, score= 0.79886\n",
      "iteration 17400, score= 0.79920\n",
      "[17500]\ttraining's binary_logloss: 0.0895392\ttraining's amex_metric: 0.991448\tvalid_1's binary_logloss: 0.213405\tvalid_1's amex_metric: 0.799319\n",
      "iteration 17500, score= 0.79930\n",
      "iteration 17600, score= 0.79904\n",
      "iteration 17700, score= 0.79899\n",
      "iteration 17800, score= 0.79904\n",
      "iteration 17900, score= 0.79915\n",
      "[18000]\ttraining's binary_logloss: 0.0875769\ttraining's amex_metric: 0.992836\tvalid_1's binary_logloss: 0.213483\tvalid_1's amex_metric: 0.799233\n",
      "iteration 18000, score= 0.79925\n",
      "iteration 18100, score= 0.79874\n",
      "iteration 18200, score= 0.79857\n",
      "iteration 18300, score= 0.79860\n",
      "iteration 18400, score= 0.79883\n",
      "[18500]\ttraining's binary_logloss: 0.0855838\ttraining's amex_metric: 0.993974\tvalid_1's binary_logloss: 0.213672\tvalid_1's amex_metric: 0.798616\n",
      "iteration 18500, score= 0.79862\n",
      "iteration 18600, score= 0.79868\n",
      "iteration 18700, score= 0.79864\n",
      "iteration 18800, score= 0.79848\n",
      "iteration 18900, score= 0.79845\n",
      "[19000]\ttraining's binary_logloss: 0.0835104\ttraining's amex_metric: 0.994921\tvalid_1's binary_logloss: 0.213797\tvalid_1's amex_metric: 0.798603\n",
      "iteration 19000, score= 0.79858\n",
      "iteration 19100, score= 0.79849\n",
      "iteration 19200, score= 0.79883\n",
      "iteration 19300, score= 0.79893\n",
      "iteration 19400, score= 0.79847\n",
      "[19500]\ttraining's binary_logloss: 0.0813101\ttraining's amex_metric: 0.995855\tvalid_1's binary_logloss: 0.213987\tvalid_1's amex_metric: 0.798635\n",
      "iteration 19500, score= 0.79864\n",
      "iteration 19600, score= 0.79870\n",
      "iteration 19700, score= 0.79886\n",
      "iteration 19800, score= 0.79916\n",
      "iteration 19900, score= 0.79870\n",
      "[20000]\ttraining's binary_logloss: 0.0793901\ttraining's amex_metric: 0.996648\tvalid_1's binary_logloss: 0.214104\tvalid_1's amex_metric: 0.798912\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50)\n",
    "print(\"Training fold {} with {} features...\".format(target_fold, len(features)))\n",
    "\n",
    "global max_score \n",
    "max_score = 0.793\n",
    "\n",
    "def save_model():\n",
    "    def callback(env):\n",
    "        global max_score\n",
    "        iteration = env.iteration\n",
    "        score = env.evaluation_result_list[3][2]\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"iteration {}, score= {:.05f}\".format(iteration,score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            print(\"High Score: iteration {}, score={:.05f}\".format(iteration, score))\n",
    "            dump(env.model, os.path.join(save_path, \"{:.05f}.pkl\".format(score)))\n",
    "\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=20000,\n",
    "    valid_sets = [lgb_train, lgb_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval = 500,\n",
    "    feval = lgb_amex_metric,\n",
    "    callbacks=[save_model()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training(train):\n",
    "    \n",
    "#     # create a numpy array to store out of folds predictions\n",
    "#     oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "#     kfold = StratifiedKFold(\n",
    "#         n_splits=n_folds, \n",
    "#         shuffle=True, \n",
    "#         random_state=seed\n",
    "#     )\n",
    "    \n",
    "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "#         print(\"#\" * 50)\n",
    "#         print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "#         x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "#         y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "        \n",
    "#         lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "#         lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "#         model = lgb.train(\n",
    "#             params=params,\n",
    "#             train_set=lgb_train,\n",
    "#             num_boost_round=10500,\n",
    "#             valid_sets = [lgb_train, lgb_valid],\n",
    "#             early_stopping_rounds=100,\n",
    "#             verbose_eval = 500,\n",
    "#             feval = lgb_amex_metric\n",
    "#         )\n",
    "#         # save best model\n",
    "#         dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "#         # predict validation\n",
    "#         val_pred = model.predict(x_val)\n",
    "        \n",
    "#         # add to out of folds array\n",
    "#         oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "#         # compute fold metric\n",
    "#         score = amex_metric(y_val, val_pred)\n",
    "#         print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "#         del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "#         gc.collect()\n",
    "        \n",
    "#     # compute oof\n",
    "#     score = amex_metric(train[target], oof_predictions)\n",
    "#     print(\"oof score is {}\".format(score))\n",
    "    \n",
    "#     # create a dataframe to store out of folds predictions\n",
    "#     oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "#     oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "#     return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
