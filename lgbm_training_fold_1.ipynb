{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/train_full_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    \n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    \n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "        \n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return \"amex_metric\", amex_metric(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_folds = 5\n",
    "\n",
    "features = load(\"selected_features.pkl\")\n",
    "\n",
    "target = \"target\"\n",
    "\n",
    "cat_features_base = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "] \n",
    "cat_features = [\n",
    "    \"{}_last\".format(feature) for feature in cat_features_base\n",
    "]\n",
    "cat_features = [feature for feature in cat_features if feature in features]\n",
    "            \n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": seed,\n",
    "    \"num_leaves\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.20,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"bagging_fraction\": 0.50,\n",
    "    \"n_jobs\": -1,\n",
    "    \"lambda_l2\": 2,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fold = 1\n",
    "\n",
    "kfold = StratifiedKFold(\n",
    "    n_splits=n_folds, \n",
    "    shuffle=True, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "    \n",
    "    if fold == target_fold:\n",
    "        break\n",
    "\n",
    "x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, x_train, x_val, y_train, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(\"../ckpt/lgbm_seed_{}\".format(seed))\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "save_path = os.path.join(save_folder, \"fold_{}\".format(target_fold))\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training fold 1 with 1950 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 18.583838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 211522\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "iteration 0, score= 0.69048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozhehan/miniconda3/envs/trade/lib/python3.8/site-packages/lightgbm/callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100, score= 0.75655\n",
      "iteration 200, score= 0.75925\n",
      "iteration 300, score= 0.76142\n",
      "iteration 400, score= 0.76296\n",
      "[500]\ttraining's binary_logloss: 0.337175\ttraining's amex_metric: 0.777825\tvalid_1's binary_logloss: 0.340452\tvalid_1's amex_metric: 0.765078\n",
      "iteration 500, score= 0.76494\n",
      "iteration 600, score= 0.76670\n",
      "iteration 700, score= 0.76874\n",
      "iteration 800, score= 0.77190\n",
      "iteration 900, score= 0.77374\n",
      "[1000]\ttraining's binary_logloss: 0.246138\ttraining's amex_metric: 0.795001\tvalid_1's binary_logloss: 0.25363\tvalid_1's amex_metric: 0.775478\n",
      "iteration 1000, score= 0.77544\n",
      "iteration 1100, score= 0.77704\n",
      "iteration 1200, score= 0.77868\n",
      "iteration 1300, score= 0.78048\n",
      "iteration 1400, score= 0.78138\n",
      "[1500]\ttraining's binary_logloss: 0.221992\ttraining's amex_metric: 0.808108\tvalid_1's binary_logloss: 0.233646\tvalid_1's amex_metric: 0.781912\n",
      "iteration 1500, score= 0.78215\n",
      "iteration 1600, score= 0.78332\n",
      "iteration 1700, score= 0.78469\n",
      "iteration 1800, score= 0.78581\n",
      "iteration 1900, score= 0.78655\n",
      "[2000]\ttraining's binary_logloss: 0.208425\ttraining's amex_metric: 0.821057\tvalid_1's binary_logloss: 0.22511\tvalid_1's amex_metric: 0.787447\n",
      "iteration 2000, score= 0.78762\n",
      "iteration 2100, score= 0.78823\n",
      "iteration 2200, score= 0.78840\n",
      "iteration 2300, score= 0.78936\n",
      "iteration 2400, score= 0.78982\n",
      "[2500]\ttraining's binary_logloss: 0.201572\ttraining's amex_metric: 0.830979\tvalid_1's binary_logloss: 0.222253\tvalid_1's amex_metric: 0.789352\n",
      "iteration 2500, score= 0.78933\n",
      "iteration 2600, score= 0.78993\n",
      "iteration 2700, score= 0.78995\n",
      "iteration 2800, score= 0.79032\n",
      "iteration 2900, score= 0.79125\n",
      "[3000]\ttraining's binary_logloss: 0.194689\ttraining's amex_metric: 0.841091\tvalid_1's binary_logloss: 0.219978\tvalid_1's amex_metric: 0.791468\n",
      "iteration 3000, score= 0.79149\n",
      "iteration 3100, score= 0.79205\n",
      "iteration 3200, score= 0.79214\n",
      "High Score: iteration 3285, score=0.79305\n",
      "High Score: iteration 3287, score=0.79305\n",
      "High Score: iteration 3288, score=0.79305\n",
      "iteration 3300, score= 0.79293\n",
      "High Score: iteration 3307, score=0.79306\n",
      "High Score: iteration 3308, score=0.79306\n",
      "High Score: iteration 3309, score=0.79307\n",
      "High Score: iteration 3311, score=0.79323\n",
      "High Score: iteration 3345, score=0.79341\n",
      "High Score: iteration 3346, score=0.79346\n",
      "High Score: iteration 3353, score=0.79350\n",
      "High Score: iteration 3356, score=0.79351\n",
      "High Score: iteration 3362, score=0.79353\n",
      "High Score: iteration 3363, score=0.79353\n",
      "High Score: iteration 3366, score=0.79354\n",
      "High Score: iteration 3368, score=0.79368\n",
      "iteration 3400, score= 0.79350\n",
      "High Score: iteration 3438, score=0.79370\n",
      "High Score: iteration 3441, score=0.79370\n",
      "High Score: iteration 3442, score=0.79372\n",
      "High Score: iteration 3452, score=0.79375\n",
      "High Score: iteration 3453, score=0.79375\n",
      "High Score: iteration 3485, score=0.79382\n",
      "High Score: iteration 3486, score=0.79388\n",
      "High Score: iteration 3487, score=0.79388\n",
      "High Score: iteration 3488, score=0.79388\n",
      "[3500]\ttraining's binary_logloss: 0.188379\ttraining's amex_metric: 0.850743\tvalid_1's binary_logloss: 0.218393\tvalid_1's amex_metric: 0.793681\n",
      "iteration 3500, score= 0.79364\n",
      "High Score: iteration 3516, score=0.79399\n",
      "High Score: iteration 3517, score=0.79399\n",
      "High Score: iteration 3518, score=0.79401\n",
      "High Score: iteration 3519, score=0.79424\n",
      "High Score: iteration 3520, score=0.79424\n",
      "High Score: iteration 3596, score=0.79430\n",
      "iteration 3600, score= 0.79419\n",
      "High Score: iteration 3632, score=0.79434\n",
      "High Score: iteration 3654, score=0.79441\n",
      "High Score: iteration 3655, score=0.79443\n",
      "High Score: iteration 3667, score=0.79448\n",
      "High Score: iteration 3668, score=0.79448\n",
      "High Score: iteration 3669, score=0.79452\n",
      "High Score: iteration 3670, score=0.79461\n",
      "iteration 3700, score= 0.79417\n",
      "High Score: iteration 3737, score=0.79461\n",
      "High Score: iteration 3738, score=0.79461\n",
      "High Score: iteration 3742, score=0.79461\n",
      "High Score: iteration 3743, score=0.79465\n",
      "High Score: iteration 3777, score=0.79467\n",
      "High Score: iteration 3786, score=0.79467\n",
      "High Score: iteration 3787, score=0.79467\n",
      "High Score: iteration 3788, score=0.79469\n",
      "High Score: iteration 3789, score=0.79471\n",
      "High Score: iteration 3790, score=0.79472\n",
      "High Score: iteration 3791, score=0.79472\n",
      "High Score: iteration 3795, score=0.79478\n",
      "High Score: iteration 3796, score=0.79478\n",
      "High Score: iteration 3797, score=0.79478\n",
      "High Score: iteration 3798, score=0.79482\n",
      "iteration 3800, score= 0.79476\n",
      "High Score: iteration 3835, score=0.79484\n",
      "iteration 3900, score= 0.79438\n",
      "[4000]\ttraining's binary_logloss: 0.182853\ttraining's amex_metric: 0.859591\tvalid_1's binary_logloss: 0.217553\tvalid_1's amex_metric: 0.794153\n",
      "iteration 4000, score= 0.79403\n",
      "iteration 4100, score= 0.79422\n",
      "iteration 4200, score= 0.79452\n",
      "iteration 4300, score= 0.79426\n",
      "iteration 4400, score= 0.79452\n",
      "High Score: iteration 4492, score=0.79485\n",
      "High Score: iteration 4494, score=0.79487\n",
      "High Score: iteration 4495, score=0.79491\n",
      "[4500]\ttraining's binary_logloss: 0.177402\ttraining's amex_metric: 0.869525\tvalid_1's binary_logloss: 0.216823\tvalid_1's amex_metric: 0.794792\n",
      "iteration 4500, score= 0.79473\n",
      "High Score: iteration 4534, score=0.79493\n",
      "High Score: iteration 4537, score=0.79497\n",
      "High Score: iteration 4539, score=0.79501\n",
      "High Score: iteration 4540, score=0.79503\n",
      "High Score: iteration 4542, score=0.79506\n",
      "High Score: iteration 4543, score=0.79506\n",
      "High Score: iteration 4544, score=0.79510\n",
      "High Score: iteration 4545, score=0.79510\n",
      "High Score: iteration 4546, score=0.79510\n",
      "iteration 4600, score= 0.79484\n",
      "iteration 4700, score= 0.79463\n",
      "High Score: iteration 4799, score=0.79511\n",
      "iteration 4800, score= 0.79515\n",
      "High Score: iteration 4800, score=0.79515\n",
      "iteration 4900, score= 0.79498\n",
      "[5000]\ttraining's binary_logloss: 0.172056\ttraining's amex_metric: 0.878342\tvalid_1's binary_logloss: 0.216222\tvalid_1's amex_metric: 0.794398\n",
      "iteration 5000, score= 0.79438\n",
      "iteration 5100, score= 0.79469\n",
      "High Score: iteration 5182, score=0.79525\n",
      "iteration 5200, score= 0.79472\n",
      "High Score: iteration 5266, score=0.79532\n",
      "High Score: iteration 5267, score=0.79536\n",
      "High Score: iteration 5270, score=0.79549\n",
      "High Score: iteration 5290, score=0.79551\n",
      "High Score: iteration 5295, score=0.79555\n",
      "iteration 5300, score= 0.79524\n",
      "High Score: iteration 5357, score=0.79559\n",
      "High Score: iteration 5389, score=0.79560\n",
      "High Score: iteration 5394, score=0.79560\n",
      "High Score: iteration 5398, score=0.79560\n",
      "High Score: iteration 5399, score=0.79560\n",
      "iteration 5400, score= 0.79560\n",
      "High Score: iteration 5400, score=0.79560\n",
      "High Score: iteration 5419, score=0.79560\n",
      "[5500]\ttraining's binary_logloss: 0.16732\ttraining's amex_metric: 0.886733\tvalid_1's binary_logloss: 0.215844\tvalid_1's amex_metric: 0.795389\n",
      "iteration 5500, score= 0.79539\n",
      "iteration 5600, score= 0.79522\n",
      "iteration 5700, score= 0.79497\n",
      "iteration 5800, score= 0.79496\n",
      "iteration 5900, score= 0.79469\n",
      "[6000]\ttraining's binary_logloss: 0.163232\ttraining's amex_metric: 0.894181\tvalid_1's binary_logloss: 0.215635\tvalid_1's amex_metric: 0.794786\n",
      "iteration 6000, score= 0.79485\n",
      "iteration 6100, score= 0.79530\n",
      "iteration 6200, score= 0.79526\n",
      "High Score: iteration 6274, score=0.79566\n",
      "High Score: iteration 6275, score=0.79568\n",
      "High Score: iteration 6276, score=0.79568\n",
      "High Score: iteration 6280, score=0.79570\n",
      "High Score: iteration 6290, score=0.79572\n",
      "High Score: iteration 6291, score=0.79572\n",
      "iteration 6300, score= 0.79524\n",
      "iteration 6400, score= 0.79501\n",
      "[6500]\ttraining's binary_logloss: 0.158956\ttraining's amex_metric: 0.900754\tvalid_1's binary_logloss: 0.21543\tvalid_1's amex_metric: 0.795165\n",
      "iteration 6500, score= 0.79516\n",
      "High Score: iteration 6540, score=0.79584\n",
      "High Score: iteration 6556, score=0.79586\n",
      "High Score: iteration 6557, score=0.79588\n",
      "iteration 6600, score= 0.79538\n",
      "High Score: iteration 6680, score=0.79592\n",
      "iteration 6700, score= 0.79576\n",
      "High Score: iteration 6725, score=0.79599\n",
      "High Score: iteration 6726, score=0.79599\n",
      "iteration 6800, score= 0.79553\n",
      "High Score: iteration 6875, score=0.79604\n",
      "High Score: iteration 6876, score=0.79608\n",
      "High Score: iteration 6889, score=0.79616\n",
      "iteration 6900, score= 0.79591\n",
      "High Score: iteration 6943, score=0.79618\n",
      "High Score: iteration 6947, score=0.79618\n",
      "High Score: iteration 6948, score=0.79625\n",
      "High Score: iteration 6949, score=0.79625\n",
      "High Score: iteration 6950, score=0.79627\n",
      "[7000]\ttraining's binary_logloss: 0.154012\ttraining's amex_metric: 0.908593\tvalid_1's binary_logloss: 0.215259\tvalid_1's amex_metric: 0.79598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7000, score= 0.79602\n",
      "iteration 7100, score= 0.79604\n",
      "iteration 7200, score= 0.79600\n",
      "iteration 7300, score= 0.79593\n",
      "iteration 7400, score= 0.79529\n",
      "[7500]\ttraining's binary_logloss: 0.149318\ttraining's amex_metric: 0.916335\tvalid_1's binary_logloss: 0.215076\tvalid_1's amex_metric: 0.795402\n",
      "iteration 7500, score= 0.79540\n",
      "iteration 7600, score= 0.79544\n",
      "iteration 7700, score= 0.79519\n",
      "iteration 7800, score= 0.79547\n",
      "iteration 7900, score= 0.79611\n",
      "High Score: iteration 7943, score=0.79629\n",
      "High Score: iteration 7946, score=0.79631\n",
      "High Score: iteration 7947, score=0.79631\n",
      "High Score: iteration 7950, score=0.79637\n",
      "High Score: iteration 7951, score=0.79639\n",
      "High Score: iteration 7953, score=0.79640\n",
      "High Score: iteration 7957, score=0.79640\n",
      "High Score: iteration 7958, score=0.79648\n",
      "High Score: iteration 7962, score=0.79661\n",
      "[8000]\ttraining's binary_logloss: 0.145121\ttraining's amex_metric: 0.92339\tvalid_1's binary_logloss: 0.214926\tvalid_1's amex_metric: 0.796161\n",
      "iteration 8000, score= 0.79620\n",
      "iteration 8100, score= 0.79606\n",
      "iteration 8200, score= 0.79594\n",
      "iteration 8300, score= 0.79594\n",
      "iteration 8400, score= 0.79631\n",
      "[8500]\ttraining's binary_logloss: 0.141562\ttraining's amex_metric: 0.929641\tvalid_1's binary_logloss: 0.214838\tvalid_1's amex_metric: 0.796088\n",
      "iteration 8500, score= 0.79609\n",
      "iteration 8600, score= 0.79592\n",
      "iteration 8700, score= 0.79616\n",
      "iteration 8800, score= 0.79606\n",
      "iteration 8900, score= 0.79602\n",
      "[9000]\ttraining's binary_logloss: 0.137518\ttraining's amex_metric: 0.93591\tvalid_1's binary_logloss: 0.214765\tvalid_1's amex_metric: 0.796281\n",
      "iteration 9000, score= 0.79628\n",
      "High Score: iteration 9052, score=0.79663\n",
      "High Score: iteration 9055, score=0.79676\n",
      "iteration 9100, score= 0.79636\n",
      "iteration 9200, score= 0.79611\n",
      "iteration 9300, score= 0.79607\n",
      "iteration 9400, score= 0.79593\n",
      "[9500]\ttraining's binary_logloss: 0.133891\ttraining's amex_metric: 0.941579\tvalid_1's binary_logloss: 0.214776\tvalid_1's amex_metric: 0.796254\n",
      "iteration 9500, score= 0.79628\n",
      "iteration 9600, score= 0.79638\n",
      "iteration 9700, score= 0.79646\n",
      "iteration 9800, score= 0.79646\n",
      "iteration 9900, score= 0.79619\n",
      "[10000]\ttraining's binary_logloss: 0.130325\ttraining's amex_metric: 0.94676\tvalid_1's binary_logloss: 0.214795\tvalid_1's amex_metric: 0.795809\n",
      "iteration 10000, score= 0.79575\n",
      "iteration 10100, score= 0.79613\n",
      "iteration 10200, score= 0.79627\n",
      "iteration 10300, score= 0.79623\n",
      "iteration 10400, score= 0.79591\n",
      "[10500]\ttraining's binary_logloss: 0.127215\ttraining's amex_metric: 0.951183\tvalid_1's binary_logloss: 0.214785\tvalid_1's amex_metric: 0.795669\n",
      "iteration 10500, score= 0.79565\n",
      "iteration 10600, score= 0.79593\n",
      "iteration 10700, score= 0.79601\n",
      "iteration 10800, score= 0.79627\n",
      "High Score: iteration 10834, score=0.79676\n",
      "iteration 10900, score= 0.79659\n",
      "High Score: iteration 10904, score=0.79676\n",
      "High Score: iteration 10905, score=0.79678\n",
      "[11000]\ttraining's binary_logloss: 0.124043\ttraining's amex_metric: 0.955304\tvalid_1's binary_logloss: 0.214772\tvalid_1's amex_metric: 0.796587\n",
      "iteration 11000, score= 0.79661\n",
      "High Score: iteration 11073, score=0.79696\n",
      "iteration 11100, score= 0.79674\n",
      "iteration 11200, score= 0.79678\n",
      "iteration 11300, score= 0.79625\n",
      "iteration 11400, score= 0.79636\n",
      "[11500]\ttraining's binary_logloss: 0.120742\ttraining's amex_metric: 0.959566\tvalid_1's binary_logloss: 0.214786\tvalid_1's amex_metric: 0.796028\n",
      "iteration 11500, score= 0.79609\n",
      "iteration 11600, score= 0.79635\n",
      "iteration 11700, score= 0.79635\n",
      "iteration 11800, score= 0.79657\n",
      "High Score: iteration 11824, score=0.79697\n",
      "iteration 11900, score= 0.79654\n",
      "[12000]\ttraining's binary_logloss: 0.117731\ttraining's amex_metric: 0.963574\tvalid_1's binary_logloss: 0.214855\tvalid_1's amex_metric: 0.796777\n",
      "iteration 12000, score= 0.79678\n",
      "iteration 12100, score= 0.79636\n",
      "iteration 12200, score= 0.79647\n",
      "High Score: iteration 12229, score=0.79701\n",
      "iteration 12300, score= 0.79637\n",
      "iteration 12400, score= 0.79631\n",
      "[12500]\ttraining's binary_logloss: 0.115128\ttraining's amex_metric: 0.967121\tvalid_1's binary_logloss: 0.214858\tvalid_1's amex_metric: 0.795858\n",
      "iteration 12500, score= 0.79586\n",
      "iteration 12600, score= 0.79633\n",
      "iteration 12700, score= 0.79637\n",
      "iteration 12800, score= 0.79609\n",
      "iteration 12900, score= 0.79634\n",
      "[13000]\ttraining's binary_logloss: 0.111784\ttraining's amex_metric: 0.970571\tvalid_1's binary_logloss: 0.214928\tvalid_1's amex_metric: 0.796792\n",
      "iteration 13000, score= 0.79671\n",
      "High Score: iteration 13029, score=0.79705\n",
      "High Score: iteration 13042, score=0.79705\n",
      "iteration 13100, score= 0.79663\n",
      "High Score: iteration 13137, score=0.79713\n",
      "High Score: iteration 13141, score=0.79715\n",
      "iteration 13200, score= 0.79670\n",
      "iteration 13300, score= 0.79693\n",
      "iteration 13400, score= 0.79683\n",
      "[13500]\ttraining's binary_logloss: 0.10909\ttraining's amex_metric: 0.974275\tvalid_1's binary_logloss: 0.214963\tvalid_1's amex_metric: 0.797127\n",
      "iteration 13500, score= 0.79717\n",
      "High Score: iteration 13500, score=0.79717\n",
      "iteration 13600, score= 0.79683\n",
      "iteration 13700, score= 0.79666\n",
      "iteration 13800, score= 0.79637\n",
      "iteration 13900, score= 0.79643\n",
      "[14000]\ttraining's binary_logloss: 0.106099\ttraining's amex_metric: 0.977387\tvalid_1's binary_logloss: 0.215058\tvalid_1's amex_metric: 0.796407\n",
      "iteration 14000, score= 0.79643\n",
      "iteration 14100, score= 0.79580\n",
      "iteration 14200, score= 0.79629\n",
      "iteration 14300, score= 0.79622\n",
      "iteration 14400, score= 0.79624\n",
      "[14500]\ttraining's binary_logloss: 0.103762\ttraining's amex_metric: 0.979595\tvalid_1's binary_logloss: 0.215134\tvalid_1's amex_metric: 0.796677\n",
      "iteration 14500, score= 0.79668\n",
      "iteration 14600, score= 0.79699\n",
      "iteration 14700, score= 0.79669\n",
      "iteration 14800, score= 0.79681\n",
      "iteration 14900, score= 0.79697\n",
      "High Score: iteration 14937, score=0.79718\n",
      "High Score: iteration 14938, score=0.79718\n",
      "High Score: iteration 14945, score=0.79718\n",
      "High Score: iteration 14948, score=0.79735\n",
      "High Score: iteration 14949, score=0.79737\n",
      "High Score: iteration 14951, score=0.79754\n",
      "High Score: iteration 14955, score=0.79754\n",
      "[15000]\ttraining's binary_logloss: 0.101388\ttraining's amex_metric: 0.982218\tvalid_1's binary_logloss: 0.215235\tvalid_1's amex_metric: 0.797268\n",
      "iteration 15000, score= 0.79723\n",
      "iteration 15100, score= 0.79706\n",
      "iteration 15200, score= 0.79737\n",
      "iteration 15300, score= 0.79724\n",
      "iteration 15400, score= 0.79702\n",
      "[15500]\ttraining's binary_logloss: 0.098898\ttraining's amex_metric: 0.984482\tvalid_1's binary_logloss: 0.215404\tvalid_1's amex_metric: 0.797253\n",
      "iteration 15500, score= 0.79725\n",
      "iteration 15600, score= 0.79713\n",
      "High Score: iteration 15688, score=0.79755\n",
      "iteration 15700, score= 0.79732\n",
      "High Score: iteration 15719, score=0.79758\n",
      "iteration 15800, score= 0.79705\n",
      "iteration 15900, score= 0.79717\n",
      "[16000]\ttraining's binary_logloss: 0.0962732\ttraining's amex_metric: 0.9866\tvalid_1's binary_logloss: 0.215506\tvalid_1's amex_metric: 0.796394\n",
      "iteration 16000, score= 0.79644\n",
      "iteration 16100, score= 0.79688\n",
      "iteration 16200, score= 0.79656\n",
      "iteration 16300, score= 0.79675\n",
      "iteration 16400, score= 0.79693\n",
      "[16500]\ttraining's binary_logloss: 0.0936518\ttraining's amex_metric: 0.988602\tvalid_1's binary_logloss: 0.215692\tvalid_1's amex_metric: 0.796941\n",
      "iteration 16500, score= 0.79692\n",
      "iteration 16600, score= 0.79691\n",
      "iteration 16700, score= 0.79666\n",
      "iteration 16800, score= 0.79675\n",
      "iteration 16900, score= 0.79696\n",
      "[17000]\ttraining's binary_logloss: 0.091508\ttraining's amex_metric: 0.990173\tvalid_1's binary_logloss: 0.215705\tvalid_1's amex_metric: 0.796873\n",
      "iteration 17000, score= 0.79685\n",
      "iteration 17100, score= 0.79674\n",
      "iteration 17200, score= 0.79674\n",
      "iteration 17300, score= 0.79675\n",
      "iteration 17400, score= 0.79685\n",
      "[17500]\ttraining's binary_logloss: 0.0891954\ttraining's amex_metric: 0.991713\tvalid_1's binary_logloss: 0.21583\tvalid_1's amex_metric: 0.796914\n",
      "iteration 17500, score= 0.79691\n",
      "iteration 17600, score= 0.79717\n",
      "iteration 17700, score= 0.79695\n",
      "iteration 17800, score= 0.79696\n",
      "iteration 17900, score= 0.79675\n",
      "[18000]\ttraining's binary_logloss: 0.0872458\ttraining's amex_metric: 0.992827\tvalid_1's binary_logloss: 0.215921\tvalid_1's amex_metric: 0.796856\n",
      "iteration 18000, score= 0.79696\n",
      "iteration 18100, score= 0.79707\n",
      "iteration 18200, score= 0.79704\n",
      "iteration 18300, score= 0.79719\n",
      "iteration 18400, score= 0.79670\n",
      "[18500]\ttraining's binary_logloss: 0.0852566\ttraining's amex_metric: 0.993865\tvalid_1's binary_logloss: 0.2161\tvalid_1's amex_metric: 0.79699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 18500, score= 0.79699\n",
      "iteration 18600, score= 0.79674\n",
      "High Score: iteration 18665, score=0.79758\n",
      "iteration 18700, score= 0.79755\n",
      "High Score: iteration 18794, score=0.79774\n",
      "iteration 18800, score= 0.79771\n",
      "High Score: iteration 18807, score=0.79778\n",
      "iteration 18900, score= 0.79734\n",
      "High Score: iteration 18941, score=0.79778\n",
      "High Score: iteration 18942, score=0.79778\n",
      "High Score: iteration 18958, score=0.79787\n",
      "High Score: iteration 18959, score=0.79787\n",
      "High Score: iteration 18962, score=0.79787\n",
      "[19000]\ttraining's binary_logloss: 0.0832011\ttraining's amex_metric: 0.994959\tvalid_1's binary_logloss: 0.216276\tvalid_1's amex_metric: 0.797764\n",
      "iteration 19000, score= 0.79772\n",
      "iteration 19100, score= 0.79766\n",
      "High Score: iteration 19112, score=0.79800\n",
      "iteration 19200, score= 0.79758\n",
      "iteration 19300, score= 0.79715\n",
      "iteration 19400, score= 0.79719\n",
      "[19500]\ttraining's binary_logloss: 0.0810271\ttraining's amex_metric: 0.995842\tvalid_1's binary_logloss: 0.216454\tvalid_1's amex_metric: 0.79751\n",
      "iteration 19500, score= 0.79751\n",
      "iteration 19600, score= 0.79715\n",
      "iteration 19700, score= 0.79708\n",
      "iteration 19800, score= 0.79670\n",
      "iteration 19900, score= 0.79704\n",
      "[20000]\ttraining's binary_logloss: 0.0790997\ttraining's amex_metric: 0.996705\tvalid_1's binary_logloss: 0.216642\tvalid_1's amex_metric: 0.797117\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50)\n",
    "print(\"Training fold {} with {} features...\".format(target_fold, len(features)))\n",
    "\n",
    "global max_score \n",
    "max_score = 0.793\n",
    "\n",
    "def save_model():\n",
    "    def callback(env):\n",
    "        global max_score\n",
    "        iteration = env.iteration\n",
    "        score = env.evaluation_result_list[3][2]\n",
    "        if iteration % 100 == 0:\n",
    "            print(\"iteration {}, score= {:.05f}\".format(iteration,score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            print(\"High Score: iteration {}, score={:.05f}\".format(iteration, score))\n",
    "            dump(env.model, os.path.join(save_path, \"{:.05f}.pkl\".format(score)))\n",
    "\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=20000,\n",
    "    valid_sets = [lgb_train, lgb_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval = 500,\n",
    "    feval = lgb_amex_metric,\n",
    "    callbacks=[save_model()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training(train):\n",
    "    \n",
    "#     # create a numpy array to store out of folds predictions\n",
    "#     oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "#     kfold = StratifiedKFold(\n",
    "#         n_splits=n_folds, \n",
    "#         shuffle=True, \n",
    "#         random_state=seed\n",
    "#     )\n",
    "    \n",
    "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[target])):\n",
    "        \n",
    "#         print(\"#\" * 50)\n",
    "#         print(\"Training fold {} with {} features...\".format(fold, len(features)))\n",
    "        \n",
    "#         x_train, x_val = train.loc[trn_ind, features], train.loc[val_ind, features]\n",
    "#         y_train, y_val = train.loc[trn_ind, target], train.loc[val_ind, target]\n",
    "        \n",
    "#         lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "#         lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "#         model = lgb.train(\n",
    "#             params=params,\n",
    "#             train_set=lgb_train,\n",
    "#             num_boost_round=10500,\n",
    "#             valid_sets = [lgb_train, lgb_valid],\n",
    "#             early_stopping_rounds=100,\n",
    "#             verbose_eval = 500,\n",
    "#             feval = lgb_amex_metric\n",
    "#         )\n",
    "#         # save best model\n",
    "#         dump(model, \"../ckpt/lgbm_{}_{}.pkl\".format(fold, seed))\n",
    "        \n",
    "#         # predict validation\n",
    "#         val_pred = model.predict(x_val)\n",
    "        \n",
    "#         # add to out of folds array\n",
    "#         oof_predictions[val_ind] = val_pred\n",
    "        \n",
    "#         # compute fold metric\n",
    "#         score = amex_metric(y_val, val_pred)\n",
    "#         print(\"fold {} score is {}\".format(fold, score))\n",
    "        \n",
    "#         del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "#         gc.collect()\n",
    "        \n",
    "#     # compute oof\n",
    "#     score = amex_metric(train[target], oof_predictions)\n",
    "#     print(\"oof score is {}\".format(score))\n",
    "    \n",
    "#     # create a dataframe to store out of folds predictions\n",
    "#     oof_df = pd.DataFrame({\"customer_ID\": train[\"customer_ID\"], \"target\": train[target], \"prediction\": oof_predictions})\n",
    "#     oof_df.to_parquet(\"lgbm_oof_{}.parquet\".format(seed))\n",
    "    \n",
    "#     return oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
