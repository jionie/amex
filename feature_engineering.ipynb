{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(\n",
    "    path=\"\", \n",
    "    usecols=None\n",
    "):\n",
    "    # LOAD DATAFRAME\n",
    "    if usecols is not None: \n",
    "        df = pd.read_parquet(path, columns=usecols)\n",
    "    else: \n",
    "        df = pd.read_parquet(path)\n",
    "    \n",
    "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
    "    df[\"customer_ID\"] = df[\"customer_ID\"].str[-16:]\n",
    "    \n",
    "    hex_to_int = lambda x: int(x, 16)\n",
    "    df[[\"customer_ID\"]] = df[[\"customer_ID\"]].applymap(lambda x: int(x, 16))\n",
    "    \n",
    "    df[\"customer_ID\"] = df[\"customer_ID\"].astype(\"int64\")\n",
    "    df[\"S_2\"] = pd.to_datetime(df[\"S_2\"])\n",
    "    \n",
    "    # SORT BY CUSTOMER AND DATE (so agg(\"last\") works correctly)\n",
    "    df = df.sort_values([\"customer_ID\", \"S_2\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # FILL NAN\n",
    "    print(\"shape of data:\", df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data...\n",
      "shape of data: (5531451, 190)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading train data...\")\n",
    "TRAIN_PATH = \"../input/amex-data-integer-dtypes-parquet-format/train.parquet\"\n",
    "train = load_file(path = TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert to nan\n",
    "train[train==-1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_136</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9223358381327749917</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>0.342033</td>\n",
       "      <td>9</td>\n",
       "      <td>0.298571</td>\n",
       "      <td>0.028331</td>\n",
       "      <td>0.506896</td>\n",
       "      <td>0.793958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823765</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9223358381327749917</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.340178</td>\n",
       "      <td>16</td>\n",
       "      <td>0.353684</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>0.505335</td>\n",
       "      <td>0.795727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.825231</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223358381327749917</td>\n",
       "      <td>2017-05-23</td>\n",
       "      <td>0.356010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.448582</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.506290</td>\n",
       "      <td>0.530133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.923707</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9223358381327749917</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>0.378665</td>\n",
       "      <td>1</td>\n",
       "      <td>0.443752</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>0.509069</td>\n",
       "      <td>0.539285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915724</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9223358381327749917</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>0.416543</td>\n",
       "      <td>1</td>\n",
       "      <td>0.463824</td>\n",
       "      <td>0.023064</td>\n",
       "      <td>0.505335</td>\n",
       "      <td>0.461935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.919373</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           customer_ID        S_2       P_2  D_39       B_1       B_2  \\\n",
       "0 -9223358381327749917 2017-03-31  0.342033     9  0.298571  0.028331   \n",
       "1 -9223358381327749917 2017-04-07  0.340178    16  0.353684  0.026975   \n",
       "2 -9223358381327749917 2017-05-23  0.356010     1  0.448582  0.026601   \n",
       "3 -9223358381327749917 2017-06-22  0.378665     1  0.443752  0.024322   \n",
       "4 -9223358381327749917 2017-07-22  0.416543     1  0.463824  0.023064   \n",
       "\n",
       "        R_1       S_3  D_41       B_3  ...  D_136  D_137  D_138  D_139  D_140  \\\n",
       "0  0.506896  0.793958   0.0  0.823765  ...    NaN    NaN    NaN    0.0    0.0   \n",
       "1  0.505335  0.795727   0.0  0.825231  ...    NaN    NaN    NaN    0.0    0.0   \n",
       "2  0.506290  0.530133   0.0  0.923707  ...    NaN    NaN    NaN    0.0    0.0   \n",
       "3  0.509069  0.539285   0.0  0.915724  ...    NaN    NaN    NaN    0.0    0.0   \n",
       "4  0.505335  0.461935   0.0  0.919373  ...    NaN    NaN    NaN    0.0    0.0   \n",
       "\n",
       "   D_141  D_142  D_143     D_144  D_145  \n",
       "0    0.0    NaN    0.0  0.004787    0.0  \n",
       "1    0.0    NaN    0.0  0.003442    0.0  \n",
       "2    0.0    NaN    0.0  0.003340    0.0  \n",
       "3    0.0    NaN    0.0  0.007556    0.0  \n",
       "4    0.0    NaN    0.0  0.005299    0.0  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5531451, 190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_observation(df):\n",
    "    \n",
    "    df[\"number_of_observations\"] = df.groupby(\"customer_ID\")[\"customer_ID\"].transform(\"count\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_observation(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add first occurance flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_first_occurance(df):\n",
    "    \n",
    "    df[\"index\"] = df.index.tolist()\n",
    "    first_occurance_index = df[[\"customer_ID\", \"index\"]].groupby(\"customer_ID\").first()[\"index\"].tolist()\n",
    "    \n",
    "    df[\"first_occurance\"] = 0\n",
    "    df.loc[df[\"index\"].isin(first_occurance_index), \"first_occurance\"] = 1\n",
    "    \n",
    "    df = df.drop([\"index\"], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_first_occurance(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nan clusters first\n",
    "cols = sorted(train.columns[2:].tolist())\n",
    "nas = train[cols].isna().sum(axis=0).reset_index(name=\"NA_count\")\n",
    "nas[\"group_count\"] = nas.loc[nas.NA_count > 0].groupby(\"NA_count\").transform(\"count\")\n",
    "clusters = nas.loc[nas.group_count > 10].sort_values([\"NA_count\",\"index\"]).groupby(\"NA_count\")[\"index\"].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_0_nan(df, cluster):\n",
    "    \n",
    "    df[\"type_0_nan\"] = 0\n",
    "    df.loc[df[cluster[0]].isnull(), \"type_0_nan\"] = 1\n",
    "    df.loc[df[\"type_0_nan\"] == 1, cluster] = df.loc[df[\"type_0_nan\"] == 1, cluster].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_type_0_nan(train, clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_1_nan(df, cluster):\n",
    "    \n",
    "    df[\"type_1_nan\"] = 0\n",
    "    df.loc[\n",
    "        (df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 1), \n",
    "        \"type_1_nan\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 0), \n",
    "        \"type_1_nan\"\n",
    "    ] = 2\n",
    "    \n",
    "    # fill type_1_nan == 1 by 0\n",
    "    df.loc[df[\"type_1_nan\"] == 1, cluster] = df.loc[df[\"type_1_nan\"] == 1, cluster].fillna(0)\n",
    "    \n",
    "    # fill type_1_nan == 0 by mean of t - 1 and t + 1\n",
    "    ffill = df[[\"customer_ID\", \"type_1_nan\"] + cluster].copy()\n",
    "    bfill = df[[\"customer_ID\", \"type_1_nan\"] + cluster].copy()\n",
    "    \n",
    "    ffill[cluster] = ffill[cluster].fillna(method=\"ffill\")\n",
    "    bfill[cluster] = bfill[cluster].fillna(method=\"bfill\")\n",
    "    \n",
    "    df.loc[df[\"type_1_nan\"] == 2, cluster] = (ffill.loc[ffill[\"type_1_nan\"] == 2, cluster] + \\\n",
    "                                              bfill.loc[ffill[\"type_1_nan\"] == 2, cluster]) / 2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_type_1_nan(train, clusters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_2_nan(df, cluster):\n",
    "    \n",
    "    df[\"type_2_nan\"] = 0\n",
    "    df.loc[\n",
    "        (df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 1), \n",
    "        \"type_2_nan\"\n",
    "    ] = 1\n",
    "    df.loc[\n",
    "        (df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 0), \n",
    "        \"type_2_nan\"\n",
    "    ] = 2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_type_2_nan(train, clusters[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add time_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_id(df):\n",
    "    \n",
    "    df[\"time_id\"] = df.groupby([\"customer_ID\"]).cumcount()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_time_id(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_feature_engineer(df):\n",
    "\n",
    "    all_cols = [c for c in list(df.columns) if c not in [\"customer_ID\", \"S_2\", \"first_occurance\"]]\n",
    "    nan_related_features = [\n",
    "        \"number_of_observations\",\n",
    "        \"type_0_nan\",\n",
    "        \"type_1_nan\",\n",
    "        \"type_2_nan\"\n",
    "    ]\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    num_features = [col for col in all_cols if col not in (cat_features + nan_related_features)]\n",
    "    \n",
    "#     print(\"process nan related features\")\n",
    "#     type_2_nan_count = df[[\"customer_ID\", clusters[2][0]]].groupby(\"customer_ID\").count().rsub(\n",
    "#         df[[\"customer_ID\", clusters[2][0]]].groupby(\"customer_ID\").size(), axis=0).rename(columns={clusters[2][0] : \"type_2_nan_count\"})\n",
    "    \n",
    "#     nan_related = df.groupby(\"customer_ID\")[nan_related_features].agg(np.max)\n",
    "#     nan_related =  pd.concat([nan_related, type_2_nan_count], axis=1)\n",
    "#     print(\"nan related features shape:\", nan_related.shape)\n",
    "    \n",
    "    print(\"process num features\")\n",
    "    num_agg = df.groupby(\"customer_ID\")[num_features].agg([\n",
    "        np.nanmean, \n",
    "        np.nanstd, \n",
    "        np.nanmin, \n",
    "        np.nanmax, \n",
    "        \"last\"\n",
    "    ])\n",
    "    num_agg.columns = [\"_\".join(x) for x in num_agg.columns]\n",
    "    print(\"num features shape:\", num_agg.shape)\n",
    "    \n",
    "    print(\"process num diff features\")\n",
    "    diff = df.groupby(\"customer_ID\")[num_features].agg(\"diff\")\n",
    "    num_diff_features = [\"{}_diff\".format(x) for x in diff.columns]\n",
    "    diff.columns = num_diff_features\n",
    "    diff = pd.concat([df[\"customer_ID\"], diff], axis=1)\n",
    "    \n",
    "    num_dff_agg = diff.groupby(\"customer_ID\")[num_diff_features].agg([\n",
    "        np.nanmean, \n",
    "        np.nanstd, \n",
    "        np.nanmin, \n",
    "        np.nanmax, \n",
    "    ])\n",
    "    num_dff_agg.columns = [\"_\".join(x) for x in num_dff_agg.columns]\n",
    "    print(\"num diff features shape:\", num_dff_agg.shape)\n",
    "    \n",
    "    print(\"process cat features\")\n",
    "    cat_agg = df.groupby(\"customer_ID\")[cat_features].agg([\"count\", \"last\", \"nunique\"])\n",
    "    cat_agg.columns = [\"_\".join(x) for x in cat_agg.columns]\n",
    "    print(\"cat features shape:\", cat_agg.shape)\n",
    "\n",
    "    df = pd.concat([nan_related, num_agg, num_dff_agg, cat_agg], axis=1)\n",
    "    print(\"shape after engineering\", df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_and_feature_engineer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df):\n",
    "    \n",
    "    targets = pd.read_csv(\"../input/train_labels.csv\")\n",
    "    \n",
    "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
    "    targets[\"customer_ID\"] = targets[\"customer_ID\"].str[-16:]\n",
    "    \n",
    "    hex_to_int = lambda x: int(x, 16)\n",
    "    targets[[\"customer_ID\"]] = targets[[\"customer_ID\"]].applymap(lambda x: int(x, 16))\n",
    "    targets[\"customer_ID\"] = targets[\"customer_ID\"].astype(\"int64\")\n",
    "    \n",
    "    targets = targets.set_index(\"customer_ID\")\n",
    "    \n",
    "    df = df.merge(targets, left_index=True, right_index=True, how=\"left\")\n",
    "    df.target = df.target.astype(\"int8\")\n",
    "\n",
    "    # NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\n",
    "    df = df.sort_index().reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df):\n",
    "    \n",
    "    cat_features_base = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ] \n",
    "    cat_features = [\n",
    "        \"{}_last\".format(feature) for feature in cat_features_base\n",
    "    ]\n",
    "    \n",
    "    for feature in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        df[feature] = encoder.fit_transform(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = label_encoding(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"../input/train_sma.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
