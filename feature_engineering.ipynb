{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(\n",
    "    path=\"\", \n",
    "    usecols=None\n",
    "):\n",
    "    # LOAD DATAFRAME\n",
    "    if usecols is not None: \n",
    "        df = pd.read_parquet(path, columns=usecols)\n",
    "    else: \n",
    "        df = pd.read_parquet(path)\n",
    "    \n",
    "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
    "    df[\"customer_ID\"] = df[\"customer_ID\"].str[-16:]\n",
    "    \n",
    "    hex_to_int = lambda x: int(x, 16)\n",
    "    df[[\"customer_ID\"]] = df[[\"customer_ID\"]].applymap(lambda x: int(x, 16))\n",
    "    \n",
    "    df[\"customer_ID\"] = df[\"customer_ID\"].astype(\"int64\")\n",
    "    df[\"S_2\"] = pd.to_datetime(df[\"S_2\"])\n",
    "    \n",
    "    # SORT BY CUSTOMER AND DATE (so agg(\"last\") works correctly)\n",
    "    df = df.sort_values([\"customer_ID\", \"S_2\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # FILL NAN\n",
    "    print(\"shape of data:\", df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data...\")\n",
    "TRAIN_PATH = \"../input/amex-data-integer-dtypes-parquet-format/train.parquet\"\n",
    "train = load_file(path = TRAIN_PATH)\n",
    "\n",
    "TEST_PATH = \"../input/amex-data-integer-dtypes-parquet-format/test.parquet\"\n",
    "test = load_file(path = TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = train.select_dtypes(include=[np.int8, np.int16, np.int32, np.int64]).columns.tolist()\n",
    "int_cols = [col for col in int_cols if col not in [\"customer_ID\"]]\n",
    "dump(int_cols, \"int_cols.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"D_39\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert to nan\n",
    "train[train==-1] = np.nan\n",
    "test[test==-1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_observation(df):\n",
    "    \n",
    "    df[\"number_of_observations\"] = df.groupby(\"customer_ID\")[\"customer_ID\"].transform(\"count\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_observation(train)\n",
    "test = add_observation(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add first occurance flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_first_occurance(df):\n",
    "    \n",
    "    df[\"index\"] = df.index.tolist()\n",
    "    first_occurance_index = df[[\"customer_ID\", \"index\"]].groupby(\"customer_ID\").first()[\"index\"].tolist()\n",
    "    \n",
    "    df[\"first_occurance\"] = 0\n",
    "    df.loc[df[\"index\"].isin(first_occurance_index), \"first_occurance\"] = 1\n",
    "    \n",
    "    df = df.drop([\"index\"], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_first_occurance(train)\n",
    "test = add_first_occurance(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nan clusters first\n",
    "cols = sorted(train.columns[2:].tolist())\n",
    "nas = train[cols].isna().sum(axis=0).reset_index(name=\"NA_count\")\n",
    "nas[\"group_count\"] = nas.loc[nas.NA_count > 0].groupby(\"NA_count\").transform(\"count\")\n",
    "clusters = nas.loc[nas.group_count > 10].sort_values([\"NA_count\",\"index\"]).groupby(\"NA_count\")[\"index\"].apply(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in clusters[2]:\n",
    "    if col in int_cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_0_nan(df, cluster):\n",
    "    \n",
    "    type_0_nan_customers = df.loc[df[cluster[0]].isnull(), \"customer_ID\"].unique().tolist()\n",
    "    df.loc[df[\"customer_ID\"].isin(type_0_nan_customers), cluster] = df.loc[df[\"customer_ID\"].isin(type_0_nan_customers), cluster].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_type_0_nan(train, clusters[0])\n",
    "test = process_type_0_nan(test, clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_1_nan(df, cluster):\n",
    "    \n",
    "    type_1_nan_customers_group_0 = df.loc[(df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 0), \"customer_ID\"].unique().tolist()\n",
    "    type_1_nan_customers_group_1 = df.loc[(df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 1), \"customer_ID\"].unique().tolist()\n",
    "    \n",
    "    # fill group 1 by 0\n",
    "    df.loc[df[\"customer_ID\"].isin(type_1_nan_customers_group_1), cluster] = \\\n",
    "        df.loc[df[\"customer_ID\"].isin(type_1_nan_customers_group_1), cluster].fillna(0)\n",
    "    \n",
    "    # fill group 0 by mean of t - 1 and t + 1\n",
    "    ffill = df[[\"customer_ID\"] + cluster].copy()\n",
    "    bfill = df[[\"customer_ID\"] + cluster].copy()\n",
    "    \n",
    "    ffill[cluster] = ffill[cluster].fillna(method=\"ffill\")\n",
    "    bfill[cluster] = bfill[cluster].fillna(method=\"bfill\")\n",
    "    \n",
    "    df.loc[df[\"customer_ID\"].isin(type_1_nan_customers_group_0), cluster] = \\\n",
    "        (ffill.loc[ffill[\"customer_ID\"].isin(type_1_nan_customers_group_0), cluster] + \\\n",
    "         bfill.loc[bfill[\"customer_ID\"].isin(type_1_nan_customers_group_0), cluster]) / 2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_type_1_nan(train, clusters[1])\n",
    "test = process_type_1_nan(test, clusters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_2_nan(df, cluster):\n",
    "    \n",
    "    type_2_nan_customers_group_0 = df.loc[(df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 0), \"customer_ID\"].unique().tolist()\n",
    "    type_2_nan_customers_group_1 = df.loc[(df[cluster[0]].isnull()) & (df[\"first_occurance\"] == 1), \"customer_ID\"].unique().tolist()\n",
    "    \n",
    "    ffill = df[[\"customer_ID\"] + cluster].copy()\n",
    "    bfill = df[[\"customer_ID\"] + cluster].copy()\n",
    "    \n",
    "    ffill[cluster] = ffill[cluster].fillna(method=\"ffill\")\n",
    "    bfill[cluster] = bfill[cluster].fillna(method=\"bfill\")\n",
    "    \n",
    "#     # fill group 1 by bfill\n",
    "#     df.loc[df[\"customer_ID\"].isin(type_2_nan_customers_group_1), cluster] = \\\n",
    "#         bfill.loc[bfill[\"customer_ID\"].isin(type_2_nan_customers_group_1), cluster]\n",
    "\n",
    "    # fill group 1 by 0\n",
    "    df.loc[df[\"customer_ID\"].isin(type_2_nan_customers_group_1), cluster] = \\\n",
    "        df.loc[df[\"customer_ID\"].isin(type_2_nan_customers_group_1), cluster].fillna(0)\n",
    "    \n",
    "    # fill group 0 by mean of fill and bfill\n",
    "    df.loc[df[\"customer_ID\"].isin(type_2_nan_customers_group_0), cluster] = \\\n",
    "        (ffill.loc[ffill[\"customer_ID\"].isin(type_2_nan_customers_group_0), cluster] + \\\n",
    "         bfill.loc[bfill[\"customer_ID\"].isin(type_2_nan_customers_group_0), cluster]) / 2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_type_2_nan(train, clusters[2])\n",
    "test = process_type_2_nan(test, clusters[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add time id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_id(df):\n",
    "    \n",
    "    df[\"time_id\"] = df.groupby([\"customer_ID\"]).cumcount()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_time_id(train)\n",
    "test = add_time_id(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add end_year_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_year_month(df):\n",
    "    \n",
    "    df[\"end_year_month\"] = df[\"S_2\"].dt.to_period(\"M\")\n",
    "    df[\"end_year_month\"] = df.groupby(\"customer_ID\")[\"end_year_month\"].transform(\"last\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_end_year_month(train)\n",
    "test = add_end_year_month(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"end_year_month\"] .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"../input/amex-data-integer-dtypes-parquet-format/train_fillna.parquet\")\n",
    "test.to_parquet(\"../input/amex-data-integer-dtypes-parquet-format/test_fillna.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = load(\"int_cols.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/amex-data-integer-dtypes-parquet-format/train_fillna.parquet\")\n",
    "test = pd.read_parquet(\"../input/amex-data-integer-dtypes-parquet-format/test_fillna.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_features = [\n",
    "    \"D_42\",\n",
    "    \"D_52\",\n",
    "    \"D_59\",\n",
    "    \"D_79\",\n",
    "    \"D_93\",\n",
    "    \"D_105\",\n",
    "    \"D_116\",\n",
    "    \"D_122\",\n",
    "    \"D_130\",\n",
    "    \"D_133\",\n",
    "    \"D_142\",\n",
    "    \"S_11\",\n",
    "    \"B_36\"\n",
    "]\n",
    "\n",
    "outlier_features = [\n",
    "    \"D_106\",\n",
    "    \"S_23\",\n",
    "    \"B_10\",\n",
    "]\n",
    "outlier_features = [feature for feature in outlier_features if feature not in int_cols]\n",
    "\n",
    "test_base_outlier_features = [\n",
    "    \"D_102\",\n",
    "    \"D_109\",\n",
    "    \"D_144\",\n",
    "    \"B_6\",\n",
    "    \"B_40\"\n",
    "]\n",
    "test_base_outlier_features = [feature for feature in test_base_outlier_features if feature not in int_cols]\n",
    "\n",
    "test_public_base_outlier_features = [\n",
    "    \"D_69\"\n",
    "]\n",
    "test_public_base_outlier_features = [feature for feature in test_public_base_outlier_features if feature not in int_cols]\n",
    "\n",
    "test_private_base_outlier_features = [\n",
    "    \"S_18\"\n",
    "]\n",
    "test_private_base_outlier_features = [feature for feature in test_private_base_outlier_features if feature not in int_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_public_end_year_month = test[\"end_year_month\"].iloc[0]\n",
    "test_private_end_year_month = test[\"end_year_month\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_59\n",
      "D_79\n",
      "D_93\n",
      "D_116\n",
      "D_122\n",
      "S_11\n"
     ]
    }
   ],
   "source": [
    "for col in shift_features:\n",
    "    if col in int_cols:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift features\n",
    "test.loc[test[\"end_year_month\"] == test_public_end_year_month, shift_features] = \\\n",
    "    test.loc[test[\"end_year_month\"] == test_public_end_year_month, shift_features] - \\\n",
    "    np.nanmean(test.loc[test[\"end_year_month\"] == test_public_end_year_month, shift_features], axis=0) + \\\n",
    "    np.nanmean(train[shift_features], axis=0)\n",
    "\n",
    "test.loc[test[\"end_year_month\"] == test_private_end_year_month, shift_features] = \\\n",
    "    test.loc[test[\"end_year_month\"] == test_private_end_year_month, shift_features] - \\\n",
    "    np.nanmean(test.loc[test[\"end_year_month\"] == test_private_end_year_month, shift_features], axis=0) + \\\n",
    "    np.nanmean(train[shift_features], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # int shift_features, floor\n",
    "# int_shift_features = [feature for feature in shift_features if feature in int_cols]\n",
    "\n",
    "# test[int_shift_features] = test[int_shift_features].fillna(-100)\n",
    "# test[int_shift_features] = np.floor(test[int_shift_features]).astype(int)\n",
    "\n",
    "# test[test==-100] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # float shift_features\n",
    "# float_shift_features = [feature for feature in shift_features if feature not in int_cols]\n",
    "\n",
    "# test.loc[test[\"end_year_month\"] == test_public_end_year_month, float_shift_features] = \\\n",
    "#     test.loc[test[\"end_year_month\"] == test_public_end_year_month, float_shift_features] - \\\n",
    "#     np.nanmean(test.loc[test[\"end_year_month\"] == test_public_end_year_month, float_shift_features], axis=0) + \\\n",
    "#     np.nanmean(train[float_shift_features], axis=0)\n",
    "\n",
    "# test.loc[test[\"end_year_month\"] == test_private_end_year_month, float_shift_features] = \\\n",
    "#     test.loc[test[\"end_year_month\"] == test_private_end_year_month, float_shift_features] - \\\n",
    "#     np.nanmean(test.loc[test[\"end_year_month\"] == test_private_end_year_month, float_shift_features], axis=0) + \\\n",
    "#     np.nanmean(train[float_shift_features], axis=0)\n",
    "\n",
    "# # int shift_features\n",
    "# int_shift_features = [feature for feature in shift_features if feature in int_cols]\n",
    "\n",
    "# test.loc[test[\"end_year_month\"] == test_public_end_year_month, int_shift_features] = \\\n",
    "#     test.loc[test[\"end_year_month\"] == test_public_end_year_month, int_shift_features] - \\\n",
    "#     np.floor(np.nanmean(test.loc[test[\"end_year_month\"] == test_public_end_year_month, int_shift_features], axis=0)).astype(int) + \\\n",
    "#     np.floor(np.nanmean(train[int_shift_features], axis=0)).astype(int)\n",
    "\n",
    "# test.loc[test[\"end_year_month\"] == test_private_end_year_month, int_shift_features] = \\\n",
    "#     test.loc[test[\"end_year_month\"] == test_private_end_year_month, int_shift_features] - \\\n",
    "#     np.floor(np.nanmean(test.loc[test[\"end_year_month\"] == test_private_end_year_month, int_shift_features], axis=0)).astype(int) + \\\n",
    "#     np.floor(np.nanmean(train[int_shift_features], axis=0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"../input/amex-data-integer-dtypes-parquet-format/train_shifted.parquet\")\n",
    "test.to_parquet(\"../input/amex-data-integer-dtypes-parquet-format/test_shifted.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../input/amex-data-integer-dtypes-parquet-format/train_shifted.parquet\")\n",
    "test = pd.read_parquet(\"../input/amex-data-integer-dtypes-parquet-format/test_shifted.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_public = test.loc[test[\"end_year_month\"] == test_public_end_year_month]\n",
    "test_private = test.loc[test[\"end_year_month\"] == test_private_end_year_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_features\n",
    "all_data = pd.concat([train, test], axis=0)\n",
    "\n",
    "outlier_features_mean, outlier_features_std = np.nanmean(all_data[outlier_features], axis=0), np.nanstd(all_data[outlier_features], axis=0)\n",
    "\n",
    "train[outlier_features] = np.clip(train[outlier_features], \n",
    "                                  outlier_features_mean - 3 * outlier_features_std, \n",
    "                                  outlier_features_mean + 3 * outlier_features_std\n",
    "                                 )\n",
    "test[outlier_features] = np.clip(test[outlier_features], \n",
    "                                 outlier_features_mean - 3 * outlier_features_std, \n",
    "                                 outlier_features_mean + 3 * outlier_features_std\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"D_109\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"D_109\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_base_outlier_features\n",
    "test_base_outlier_features_mean, test_base_outlier_features_std = \\\n",
    "    np.nanmean(test[test_base_outlier_features], axis=0), np.nanstd(test[test_base_outlier_features], axis=0)\n",
    "\n",
    "train[test_base_outlier_features] = np.clip(train[test_base_outlier_features], \n",
    "                                  test_base_outlier_features_mean - 3 * test_base_outlier_features_std, \n",
    "                                  test_base_outlier_features_mean + 3 * test_base_outlier_features_std\n",
    "                                 )\n",
    "test[test_base_outlier_features] = np.clip(test[test_base_outlier_features], \n",
    "                                 test_base_outlier_features_mean - 3 * test_base_outlier_features_std, \n",
    "                                 test_base_outlier_features_mean + 3 * test_base_outlier_features_std\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_public_base_outlier_features\n",
    "test_public_base_outlier_features_mean, test_public_base_outlier_features_std = \\\n",
    "    np.nanmean(test_public[test_public_base_outlier_features], axis=0), np.nanstd(test_public[test_public_base_outlier_features], axis=0)\n",
    "\n",
    "train[test_public_base_outlier_features] = np.clip(train[test_public_base_outlier_features], \n",
    "                                  test_public_base_outlier_features_mean - 3 * test_public_base_outlier_features_std, \n",
    "                                  test_public_base_outlier_features_mean + 3 * test_public_base_outlier_features_std\n",
    "                                 )\n",
    "test[test_public_base_outlier_features] = np.clip(test[test_public_base_outlier_features], \n",
    "                                 test_public_base_outlier_features_mean - 3 * test_public_base_outlier_features_std, \n",
    "                                 test_public_base_outlier_features_mean + 3 * test_public_base_outlier_features_std\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_private_base_outlier_features\n",
    "test_private_base_outlier_features_mean, test_private_base_outlier_features_std = \\\n",
    "    np.nanmean(test_private[test_private_base_outlier_features], axis=0), np.nanstd(test_private[test_private_base_outlier_features], axis=0)\n",
    "\n",
    "train[test_private_base_outlier_features] = np.clip(train[test_private_base_outlier_features], \n",
    "                                  test_private_base_outlier_features_mean - 3 * test_private_base_outlier_features_std, \n",
    "                                  test_private_base_outlier_features_mean + 3 * test_private_base_outlier_features_std\n",
    "                                 )\n",
    "test[test_private_base_outlier_features] = np.clip(test[test_private_base_outlier_features], \n",
    "                                 test_private_base_outlier_features_mean - 3 * test_private_base_outlier_features_std, \n",
    "                                 test_private_base_outlier_features_mean + 3 * test_private_base_outlier_features_std\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base_outlier_features_mean[1], test_base_outlier_features_std[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"../input/amex-data-integer-dtypes-parquet-format/train_clipped.parquet\")\n",
    "test.to_parquet(\"../input/amex-data-integer-dtypes-parquet-format/test_clipped.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_feature_engineer(df):\n",
    "\n",
    "    all_cols = [c for c in list(df.columns) if c not in [\"customer_ID\", \"S_2\", \"first_occurance\", \"time_id\", \"end_year_month\"]]\n",
    "    nan_related_features = [\n",
    "        \"number_of_observations\",\n",
    "        \"type_0_nan\",\n",
    "        \"type_1_nan\",\n",
    "        \"type_2_nan\"\n",
    "    ]\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    num_features = [col for col in all_cols if col not in (cat_features + nan_related_features)]\n",
    "    \n",
    "    print(\"process num features\")\n",
    "    num_agg = df.groupby(\"customer_ID\")[num_features].agg([ \n",
    "        np.nanstd, \n",
    "        np.nanmin, \n",
    "        np.nanmax,\n",
    "        \"last\"\n",
    "    ])\n",
    "    num_agg.columns = [\"_\".join(x) for x in num_agg.columns]\n",
    "    print(\"num features shape:\", num_agg.shape)\n",
    "    \n",
    "    print(\"process sma num features\")\n",
    "    sma_num_agg_0 = df.loc[df[\"time_id\"] >= 0].groupby(\"customer_ID\")[num_features].agg(np.nanmean)\n",
    "    sma_num_agg_0.columns = [(x + \"_nanmean_0\") for x in sma_num_agg_0.columns]\n",
    "    \n",
    "    sma_num_agg_4 = df.loc[df[\"time_id\"] >= 4].groupby(\"customer_ID\")[num_features].agg(np.nanmean)\n",
    "    sma_num_agg_4.columns = [(x + \"_nanmean_4\") for x in sma_num_agg_4.columns]\n",
    "    \n",
    "    sma_num_agg_7 = df.loc[df[\"time_id\"] >= 7].groupby(\"customer_ID\")[num_features].agg(np.nanmean)\n",
    "    sma_num_agg_7.columns = [(x + \"_nanmean_7\") for x in sma_num_agg_7.columns]\n",
    "    \n",
    "    sma_num_agg_10 = df.loc[df[\"time_id\"] >= 10].groupby(\"customer_ID\")[num_features].agg(np.nanmean)\n",
    "    sma_num_agg_10.columns = [(x + \"_nanmean_10\") for x in sma_num_agg_10.columns]\n",
    "    \n",
    "    sma_num_agg = pd.concat([sma_num_agg_0, sma_num_agg_4, sma_num_agg_7, sma_num_agg_10], axis=1)\n",
    "    print(\"sma num features shape:\", sma_num_agg.shape)\n",
    "    \n",
    "    print(\"process cat features\")\n",
    "    cat_agg = df.groupby(\"customer_ID\")[cat_features].agg([\"count\", \"last\", \"nunique\"])\n",
    "    cat_agg.columns = [\"_\".join(x) for x in cat_agg.columns]\n",
    "    print(\"cat features shape:\", cat_agg.shape)\n",
    "    \n",
    "    df = pd.concat([num_agg, sma_num_agg, cat_agg], axis=1)\n",
    "    print(\"shape after engineering\", df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_and_feature_engineer(train)\n",
    "test = process_and_feature_engineer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df):\n",
    "    \n",
    "    targets = pd.read_csv(\"../input/train_labels.csv\")\n",
    "    \n",
    "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
    "    targets[\"customer_ID\"] = targets[\"customer_ID\"].str[-16:]\n",
    "    \n",
    "    hex_to_int = lambda x: int(x, 16)\n",
    "    targets[[\"customer_ID\"]] = targets[[\"customer_ID\"]].applymap(lambda x: int(x, 16))\n",
    "    targets[\"customer_ID\"] = targets[\"customer_ID\"].astype(\"int64\")\n",
    "    \n",
    "    targets = targets.set_index(\"customer_ID\")\n",
    "    \n",
    "    df = df.merge(targets, left_index=True, right_index=True, how=\"left\")\n",
    "    df.target = df.target.astype(\"int8\")\n",
    "\n",
    "    # NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\n",
    "    df = df.sort_index().reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df):\n",
    "    \n",
    "    cat_features_base = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ] \n",
    "    cat_features = [\n",
    "        \"{}_last\".format(feature) for feature in cat_features_base\n",
    "    ]\n",
    "    \n",
    "    for feature in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        df[feature] = encoder.fit_transform(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = label_encoding(train)\n",
    "test = label_encoding(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"../input/train_base_clipped.parquet\")\n",
    "test.to_parquet(\"../input/test_base_clipped.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
